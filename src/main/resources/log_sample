/Users/Elad.Gazit/Library/Java/JavaVirtualMachines/jdk-11.0.2.jdk/Contents/Home/bin/java -javaagent:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar=52500:/Applications/IntelliJ IDEA.app/Contents/bin -Dfile.encoding=UTF-8 -classpath /Users/Elad.Gazit/IdeaProjects/table_splitter/target/scala-2.13/classes:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/clearspring/analytics/stream/2.9.6/stream-2.9.6.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.12.3/jackson-annotations-2.12.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.12.3/jackson-core-2.12.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.12.3/jackson-databind-2.12.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.13/2.12.3/jackson-module-scala_2.13-2.12.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.0-4/zstd-jni-1.5.0-4.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/google/code/gson/gson/2.8.6/gson-2.8.6.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/google/crypto/tink/tink/1.6.0/tink-1.6.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/google/guava/guava/16.0.1/guava-16.0.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.14.0/protobuf-java-3.14.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/tdunning/json/1.8/json-1.8.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/twitter/chill-java/0.10.0/chill-java-0.10.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/twitter/chill_2.13/0.10.0/chill_2.13-0.10.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/univocity/univocity-parsers/2.9.1/univocity-parsers-2.9.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/zaxxer/HikariCP/2.5.1/HikariCP-2.5.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-io/commons-io/2.8.0/commons-io-2.8.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/airlift/aircompressor/0.21/aircompressor-0.21.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.2.0/metrics-core-4.2.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/4.2.0/metrics-graphite-4.2.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-jmx/4.2.0/metrics-jmx-4.2.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-json/4.2.0/metrics-json-4.2.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/dropwizard/metrics/metrics-jvm/4.2.0/metrics-jvm-4.2.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-all/4.1.68.Final/netty-all-4.1.68.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-buffer/4.1.50.Final/netty-buffer-4.1.50.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-codec/4.1.50.Final/netty-codec-4.1.50.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-common/4.1.50.Final/netty-common-4.1.50.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-handler/4.1.50.Final/netty-handler-4.1.50.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-resolver/4.1.50.Final/netty-resolver-4.1.50.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-transport-native-epoll/4.1.50.Final/netty-transport-native-epoll-4.1.50.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-transport-native-unix-common/4.1.50.Final/netty-transport-native-unix-common-4.1.50.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-transport/4.1.50.Final/netty-transport-4.1.50.Final.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/jakarta/servlet/jakarta.servlet-api/4.0.3/jakarta.servlet-api-4.0.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/jakarta/ws/rs/jakarta.ws.rs-api/2.1.6/jakarta.ws.rs-api-2.1.6.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/javax/transaction/jta/1.1/jta-1.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/javax/transaction/transaction-api/1.1/transaction-api-1.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/javolution/javolution/5.5.1/javolution-5.5.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.12/jline-2.12.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/joda-time/joda-time/2.10.10/joda-time-2.10.10.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/net/razorvine/pyrolite/4.30/pyrolite-4.30.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/net/sf/py4j/py4j/0.10.9.3/py4j-0.10.9.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-format/2.0.0/arrow-format-2.0.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-memory-core/2.0.0/arrow-memory-core-2.0.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-memory-netty/2.0.0/arrow-memory-netty-2.0.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/arrow/arrow-vector/2.0.0/arrow-vector-2.0.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/avro/avro-ipc/1.10.2/avro-ipc-1.10.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.10.2/avro-mapred-1.10.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/avro/avro/1.10.2/avro-1.10.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-compress/1.20/commons-compress-1.20.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-crypto/1.1.0/commons-crypto-1.1.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/commons/commons-text/1.6/commons-text-1.6.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.1/hadoop-client-api-3.3.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.1/hadoop-client-runtime-3.3.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/shims/hive-shims-0.23/2.3.9/hive-shims-0.23-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/shims/hive-shims-common/2.3.9/hive-shims-common-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/shims/hive-shims-scheduler/2.3.9/hive-shims-scheduler-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-common/2.3.9/hive-common-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-exec/2.3.9/hive-exec-2.3.9-core.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-llap-client/2.3.9/hive-llap-client-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-llap-common/2.3.9/hive-llap-common-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-metastore/2.3.9/hive-metastore-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-serde/2.3.9/hive-serde-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-shims/2.3.9/hive-shims-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-storage-api/2.7.2/hive-storage-api-2.7.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/hive/hive-vector-code-gen/2.3.9/hive-vector-code-gen-2.3.9.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/ivy/ivy/2.5.0/ivy-2.5.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-core/1.6.12/orc-core-1.6.12.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-mapreduce/1.6.12/orc-mapreduce-1.6.12.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/orc/orc-shims/1.6.12/orc-shims-1.6.12.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-column/1.12.2/parquet-column-1.12.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-common/1.12.2/parquet-common-1.12.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-encoding/1.12.2/parquet-encoding-1.12.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-format-structures/1.12.2/parquet-format-structures-1.12.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop/1.12.2/parquet-hadoop-1.12.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/parquet/parquet-jackson/1.12.2/parquet-jackson-1.12.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.13/3.2.1/spark-catalyst_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-core_2.13/3.2.1/spark-core_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-hive_2.13/3.2.1/spark-hive_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-kvstore_2.13/3.2.1/spark-kvstore_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-launcher_2.13/3.2.1/spark-launcher_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-network-common_2.13/3.2.1/spark-network-common_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-network-shuffle_2.13/3.2.1/spark-network-shuffle_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-sketch_2.13/3.2.1/spark-sketch_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-sql_2.13/3.2.1/spark-sql_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-tags_2.13/3.2.1/spark-tags_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.13/3.2.1/spark-unsafe_2.13-3.2.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/thrift/libthrift/0.12.0/libthrift-0.12.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/xbean/xbean-asm9-shaded/4.20/xbean-asm9-shaded-4.20.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/yetus/audience-annotations/0.12.0/audience-annotations-0.12.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/zookeeper/zookeeper-jute/3.6.2/zookeeper-jute-3.6.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.6.2/zookeeper-3.6.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/codehaus/janino/commons-compiler/3.0.16/commons-compiler-3.0.16.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/codehaus/janino/janino/3.0.16/janino-3.0.16.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/datanucleus/datanucleus-api-jdo/4.2.4/datanucleus-api-jdo-4.2.4.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/datanucleus/datanucleus-core/4.1.17/datanucleus-core-4.1.17.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/datanucleus/datanucleus-rdbms/4.1.19/datanucleus-rdbms-4.1.19.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/datanucleus/javax.jdo/3.2.0-m3/javax.jdo-3.2.0-m3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/external/aopalliance-repackaged/2.6.1/aopalliance-repackaged-2.6.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/external/jakarta.inject/2.6.1/jakarta.inject-2.6.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-api/2.6.1/hk2-api-2.6.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-locator/2.6.1/hk2-locator-2.6.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/hk2-utils/2.6.1/hk2-utils-2.6.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/hk2/osgi-resource-locator/1.0.3/osgi-resource-locator-1.0.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.34/jersey-container-servlet-core-2.34.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet/2.34/jersey-container-servlet-2.34.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-client/2.34/jersey-client-2.34.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-common/2.34/jersey-common-2.34.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-server/2.34/jersey-server-2.34.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/glassfish/jersey/inject/jersey-hk2/2.34/jersey-hk2-2.34.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.25.0-GA/javassist-3.25.0-GA.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/json4s/json4s-ast_2.13/3.7.0-M11/json4s-ast_2.13-3.7.0-M11.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/json4s/json4s-core_2.13/3.7.0-M11/json4s-core_2.13-3.7.0-M11.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/json4s/json4s-jackson_2.13/3.7.0-M11/json4s-jackson_2.13-3.7.0-M11.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/json4s/json4s-scalap_2.13/3.7.0-M11/json4s-scalap_2.13-3.7.0-M11.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/roaringbitmap/RoaringBitmap/0.9.0/RoaringBitmap-0.9.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/roaringbitmap/shims/0.9.0/shims-0.9.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/rocksdb/rocksdbjni/6.20.3/rocksdbjni-6.20.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.0.3/scala-parallel-collections_2.13-1.0.3.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.13/1.1.2/scala-parser-combinators_2.13-1.1.2.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.13/1.2.0/scala-xml_2.13-1.2.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.13.8/scala-library-2.13.8.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.13.8/scala-reflect-2.13.8.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/slf4j/jcl-over-slf4j/1.7.30/jcl-over-slf4j-1.7.30.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/slf4j/jul-to-slf4j/1.7.30/jul-to-slf4j-1.7.30.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/tukaani/xz/1.8/xz-1.8.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/oro/oro/2.0.8/oro-2.0.8.jar:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar data.SplitterApplication
log4j:WARN No appenders could be found for logger (org.apache.hadoop.hive.conf.HiveConf).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/02/19 10:50:53 WARN Utils: Your hostname, Elads-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.101 instead (on interface en0)
22/02/19 10:50:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/Elad.Gazit/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.13/3.2.1/spark-unsafe_2.13-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
22/02/19 10:50:53 INFO SparkContext: Running Spark version 3.2.1
22/02/19 10:50:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/02/19 10:50:53 INFO ResourceUtils: ==============================================================
22/02/19 10:50:53 INFO ResourceUtils: No custom resources configured for spark.driver.
22/02/19 10:50:53 INFO ResourceUtils: ==============================================================
22/02/19 10:50:53 INFO SparkContext: Submitted application: 9b2953c3-5cd9-4441-a865-27cf9d6569e6
22/02/19 10:50:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/02/19 10:50:53 INFO ResourceProfile: Limiting resource is cpu
22/02/19 10:50:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/02/19 10:50:53 INFO SecurityManager: Changing view acls to: Elad.Gazit
22/02/19 10:50:53 INFO SecurityManager: Changing modify acls to: Elad.Gazit
22/02/19 10:50:53 INFO SecurityManager: Changing view acls groups to:
22/02/19 10:50:53 INFO SecurityManager: Changing modify acls groups to:
22/02/19 10:50:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Elad.Gazit); groups with view permissions: Set(); users  with modify permissions: Set(Elad.Gazit); groups with modify permissions: Set()
22/02/19 10:50:53 INFO Utils: Successfully started service 'sparkDriver' on port 52504.
22/02/19 10:50:53 INFO SparkEnv: Registering MapOutputTracker
22/02/19 10:50:53 INFO SparkEnv: Registering BlockManagerMaster
22/02/19 10:50:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/02/19 10:50:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/02/19 10:50:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/02/19 10:50:53 INFO DiskBlockManager: Created local directory at /private/var/folders/nw/3qtbss3x3nb2l_1wv2jltw1r0000gq/T/blockmgr-8f61ac16-6208-4e97-8d58-13f7d95d8924
22/02/19 10:50:53 INFO MemoryStore: MemoryStore started with capacity 4.6 GiB
22/02/19 10:50:53 INFO SparkEnv: Registering OutputCommitCoordinator
22/02/19 10:50:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/02/19 10:50:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.101:4040
22/02/19 10:50:54 INFO Executor: Starting executor ID driver on host 192.168.1.101
22/02/19 10:50:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52505.
22/02/19 10:50:54 INFO NettyBlockTransferService: Server created on 192.168.1.101:52505
22/02/19 10:50:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/02/19 10:50:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.101, 52505, None)
22/02/19 10:50:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.101:52505 with 4.6 GiB RAM, BlockManagerId(driver, 192.168.1.101, 52505, None)
22/02/19 10:50:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.101, 52505, None)
22/02/19 10:50:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.101, 52505, None)
22/02/19 10:50:54 INFO TableSplitter: drop database if exists mixedtiles cascade
22/02/19 10:50:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
22/02/19 10:50:54 INFO SharedState: Warehouse path is 'file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse'.
22/02/19 10:50:56 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
22/02/19 10:50:56 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse
22/02/19 10:50:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
22/02/19 10:50:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
22/02/19 10:50:56 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
22/02/19 10:50:56 INFO ObjectStore: ObjectStore, initialize called
22/02/19 10:50:56 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
22/02/19 10:50:56 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
22/02/19 10:50:57 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
22/02/19 10:50:58 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
22/02/19 10:50:58 INFO ObjectStore: Initialized ObjectStore
22/02/19 10:50:58 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
22/02/19 10:50:58 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore Elad.Gazit@127.0.0.1
22/02/19 10:50:58 INFO HiveMetaStore: Added admin role in metastore
22/02/19 10:50:58 INFO HiveMetaStore: Added public role in metastore
22/02/19 10:50:58 INFO HiveMetaStore: No user is added in admin role, since config is empty
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_database: default
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: default
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_database: global_temp
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: global_temp
22/02/19 10:50:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_tables: db=mixedtiles pat=*
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_tables: db=mixedtiles pat=*
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_all_tables: db=mixedtiles
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_all_tables: db=mixedtiles
22/02/19 10:50:58 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=contact_support
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=contact_support
22/02/19 10:50:58 INFO HiveMetaStore: 0: drop_table : db=mixedtiles tbl=contact_support
22/02/19 10:50:58 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=drop_table : db=mixedtiles tbl=contact_support
22/02/19 10:50:59 INFO deprecation: org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
22/02/19 10:50:59 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:50:59 INFO HiveMetaStore: 0: drop_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=drop_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:50:59 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:50:59 INFO HiveMetaStore: 0: drop_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=drop_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:50:59 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=order_completed
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=order_completed
22/02/19 10:50:59 INFO HiveMetaStore: 0: drop_table : db=mixedtiles tbl=order_completed
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=drop_table : db=mixedtiles tbl=order_completed
22/02/19 10:50:59 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=signup
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=signup
22/02/19 10:50:59 INFO HiveMetaStore: 0: drop_table : db=mixedtiles tbl=signup
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=drop_table : db=mixedtiles tbl=signup
22/02/19 10:50:59 INFO HiveMetaStore: 0: drop_database: mixedtiles
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=drop_database: mixedtiles
22/02/19 10:50:59 INFO HiveMetaStore: 0: get_all_tables: db=mixedtiles
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_all_tables: db=mixedtiles
22/02/19 10:50:59 INFO HiveMetaStore: 0: get_functions: db=mixedtiles pat=*
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_functions: db=mixedtiles pat=*
22/02/19 10:50:59 INFO ObjectStore: Dropping database mixedtiles along with all tables
22/02/19 10:50:59 INFO TxnHandler: START========"HiveConf()"========
hiveDefaultUrl=null
hiveSiteURL=null
hiveServer2SiteUrl=null
hivemetastoreSiteUrl=null
Values omitted for security reason if present: [fs.s3n.awsAccessKeyId, fs.s3a.access.key, fs.s3.awsAccessKeyId, hive.server2.keystore.password, fs.s3a.proxy.password, javax.jdo.option.ConnectionPassword, fs.s3.awsSecretAccessKey, fs.s3n.awsSecretAccessKey, fs.s3a.secret.key]
adl.feature.ownerandgroup.enableupn=false
adl.http.timeout=-1
datanucleus.autoStartMechanismMode=ignored
datanucleus.cache.level2=false
datanucleus.cache.level2.type=none
datanucleus.connectionPool.maxPoolSize=10
datanucleus.connectionPoolingType=BONECP
datanucleus.identifierFactory=datanucleus1
datanucleus.plugin.pluginRegistryBundleCheck=LOG
datanucleus.rdbms.initializeColumnInfo=NONE
datanucleus.rdbms.useLegacyNativeValueStrategy=true
datanucleus.schema.autoCreateAll=true
datanucleus.schema.validateColumns=false
datanucleus.schema.validateConstraints=false
datanucleus.schema.validateTables=false
datanucleus.storeManagerType=rdbms
datanucleus.transactionIsolation=read-committed
dfs.client.ignore.namenode.default.kms.uri=false
dfs.ha.fencing.ssh.connect-timeout=30000
file.blocksize=67108864
file.bytes-per-checksum=512
file.client-write-packet-size=65536
file.replication=1
file.stream-buffer-size=4096
fs.AbstractFileSystem.abfs.impl=org.apache.hadoop.fs.azurebfs.Abfs
fs.AbstractFileSystem.abfss.impl=org.apache.hadoop.fs.azurebfs.Abfss
fs.AbstractFileSystem.adl.impl=org.apache.hadoop.fs.adl.Adl
fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs
fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs
fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A
fs.AbstractFileSystem.swebhdfs.impl=org.apache.hadoop.fs.SWebHdfs
fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
fs.AbstractFileSystem.wasb.impl=org.apache.hadoop.fs.azure.Wasb
fs.AbstractFileSystem.wasbs.impl=org.apache.hadoop.fs.azure.Wasbs
fs.AbstractFileSystem.webhdfs.impl=org.apache.hadoop.fs.WebHdfs
fs.abfs.impl=org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.abfss.impl=org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
fs.adl.impl=org.apache.hadoop.fs.adl.AdlFileSystem
fs.adl.oauth2.access.token.provider.type=ClientCredential
fs.automatic.close=true
fs.azure.authorization=false
fs.azure.authorization.caching.enable=true
fs.azure.local.sas.key.mode=false
fs.azure.sas.expiry.period=90d
fs.azure.saskey.usecontainersaskeyforallaccess=true
fs.azure.secure.mode=false
fs.azure.user.agent.prefix=unknown
fs.client.resolve.remote.symlinks=true
fs.client.resolve.topology.enabled=false
fs.defaultFS=file:///
fs.df.interval=60000
fs.du.interval=600000
fs.ftp.data.connection.mode=ACTIVE_LOCAL_DATA_CONNECTION_MODE
fs.ftp.host=0.0.0.0
fs.ftp.host.port=21
fs.ftp.impl=org.apache.hadoop.fs.ftp.FTPFileSystem
fs.ftp.timeout=0
fs.ftp.transfer.mode=BLOCK_TRANSFER_MODE
fs.getspaceused.jitterMillis=60000
fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem
fs.har.impl.disable.cache=true
fs.permissions.umask-mode=022
fs.s3a.assumed.role.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
fs.s3a.assumed.role.session.duration=30m
fs.s3a.attempts.maximum=20
fs.s3a.aws.credentials.provider=
    org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,
    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,
    com.amazonaws.auth.EnvironmentVariableCredentialsProvider,
    org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider

fs.s3a.block.size=32M
fs.s3a.buffer.dir=/tmp/hadoop-Elad.Gazit/s3a
fs.s3a.change.detection.mode=server
fs.s3a.change.detection.source=etag
fs.s3a.change.detection.version.required=true
fs.s3a.committer.abort.pending.uploads=true
fs.s3a.committer.magic.enabled=true
fs.s3a.committer.name=file
fs.s3a.committer.staging.conflict-mode=append
fs.s3a.committer.staging.tmp.path=
    tmp/staging:
fs.s3a.committer.staging.unique-filenames=true
fs.s3a.committer.threads=8
fs.s3a.connection.establish.timeout=5000
fs.s3a.connection.maximum=48
fs.s3a.connection.request.timeout=0
fs.s3a.connection.ssl.enabled=true
fs.s3a.connection.timeout=200000
fs.s3a.delegation.tokens.enabled=false
fs.s3a.downgrade.syncable.exceptions=true
fs.s3a.endpoint=s3.amazonaws.com
fs.s3a.etag.checksum.enabled=false
fs.s3a.executor.capacity=16
fs.s3a.fast.upload.active.blocks=4
fs.s3a.fast.upload.buffer=disk
fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
fs.s3a.list.version=2
fs.s3a.max.total.tasks=32
fs.s3a.metadatastore.authoritative=false
fs.s3a.metadatastore.fail.on.write.error=true
fs.s3a.metadatastore.impl=org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore
fs.s3a.metadatastore.metadata.ttl=15m
fs.s3a.multiobjectdelete.enable=true
fs.s3a.multipart.purge=false
fs.s3a.multipart.purge.age=86400
fs.s3a.multipart.size=64M
fs.s3a.multipart.threshold=128M
fs.s3a.paging.maximum=5000
fs.s3a.path.style.access=
    false:
fs.s3a.readahead.range=64K
fs.s3a.retry.interval=500ms
fs.s3a.retry.limit=7
fs.s3a.retry.throttle.interval=100ms
fs.s3a.retry.throttle.limit=20
fs.s3a.s3guard.cli.prune.age=86400000
fs.s3a.s3guard.consistency.retry.interval=2s
fs.s3a.s3guard.consistency.retry.limit=7
fs.s3a.s3guard.ddb.background.sleep=25ms
fs.s3a.s3guard.ddb.max.retries=9
fs.s3a.s3guard.ddb.table.capacity.read=0
fs.s3a.s3guard.ddb.table.capacity.write=0
fs.s3a.s3guard.ddb.table.create=false
fs.s3a.s3guard.ddb.table.sse.enabled=false
fs.s3a.s3guard.ddb.throttle.retry.interval=100ms
fs.s3a.select.enabled=true
fs.s3a.select.errors.include.sql=false
fs.s3a.select.input.compression=none
fs.s3a.select.input.csv.comment.marker=#
fs.s3a.select.input.csv.field.delimiter=,
fs.s3a.select.input.csv.header=none
fs.s3a.select.input.csv.quote.character="
fs.s3a.select.input.csv.quote.escape.character=\\
fs.s3a.select.input.csv.record.delimiter=\n
fs.s3a.select.output.csv.field.delimiter=,
fs.s3a.select.output.csv.quote.character="
fs.s3a.select.output.csv.quote.escape.character=\\
fs.s3a.select.output.csv.quote.fields=always
fs.s3a.select.output.csv.record.delimiter=\n
fs.s3a.socket.recv.buffer=8192
fs.s3a.socket.send.buffer=8192
fs.s3a.ssl.channel.mode=default_jsse
fs.s3a.threads.keepalivetime=60
fs.s3a.threads.max=64
fs.scheme.class=dfs
fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
fs.trash.checkpoint.interval=0
fs.trash.interval=0
fs.viewfs.overload.scheme.target.abfs.impl=org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.viewfs.overload.scheme.target.abfss.impl=org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
fs.viewfs.overload.scheme.target.file.impl=org.apache.hadoop.fs.LocalFileSystem
fs.viewfs.overload.scheme.target.ftp.impl=org.apache.hadoop.fs.ftp.FTPFileSystem
fs.viewfs.overload.scheme.target.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem
fs.viewfs.overload.scheme.target.http.impl=org.apache.hadoop.fs.http.HttpFileSystem
fs.viewfs.overload.scheme.target.https.impl=org.apache.hadoop.fs.http.HttpsFileSystem
fs.viewfs.overload.scheme.target.o3fs.impl=org.apache.hadoop.fs.ozone.OzoneFileSystem
fs.viewfs.overload.scheme.target.ofs.impl=org.apache.hadoop.fs.ozone.RootedOzoneFileSystem
fs.viewfs.overload.scheme.target.oss.impl=org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem
fs.viewfs.overload.scheme.target.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
fs.viewfs.overload.scheme.target.swebhdfs.impl=org.apache.hadoop.hdfs.web.SWebHdfsFileSystem
fs.viewfs.overload.scheme.target.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
fs.viewfs.overload.scheme.target.wasb.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.viewfs.overload.scheme.target.webhdfs.impl=org.apache.hadoop.hdfs.web.WebHdfsFileSystem
fs.viewfs.rename.strategy=SAME_MOUNTPOINT
fs.wasb.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.wasbs.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure
ftp.blocksize=67108864
ftp.bytes-per-checksum=512
ftp.client-write-packet-size=65536
ftp.replication=3
ftp.stream-buffer-size=4096
ha.failover-controller.active-standby-elector.zk.op.retries=3
ha.failover-controller.cli-check.rpc-timeout.ms=20000
ha.failover-controller.graceful-fence.connection.retries=1
ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
ha.failover-controller.new-active.rpc-timeout.ms=60000
ha.health-monitor.check-interval.ms=1000
ha.health-monitor.connect-retry-interval.ms=1000
ha.health-monitor.rpc-timeout.ms=45000
ha.health-monitor.rpc.connect.max.retries=1
ha.health-monitor.sleep-after-disconnect.ms=1000
ha.zookeeper.acl=world:anyone:rwcda
ha.zookeeper.parent-znode=/hadoop-ha
ha.zookeeper.session-timeout.ms=10000
hadoop.bin.path=
    /usr/bin/hadoop:
hadoop.caller.context.enabled=false
hadoop.caller.context.max.size=128
hadoop.caller.context.signature.max.size=40
hadoop.common.configuration.version=3.0.0
hadoop.domainname.resolver.impl=org.apache.hadoop.net.DNSDomainNameResolver
hadoop.http.authentication.kerberos.keytab=/Users/Elad.Gazit/hadoop.keytab
hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
hadoop.http.authentication.signature.secret.file=/Users/Elad.Gazit/hadoop-http-auth-signature-secret
hadoop.http.authentication.simple.anonymous.allowed=true
hadoop.http.authentication.token.validity=36000
hadoop.http.authentication.type=simple
hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin
hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD
hadoop.http.cross-origin.allowed-origins=*
hadoop.http.cross-origin.enabled=false
hadoop.http.cross-origin.max-age=1800
hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter
hadoop.http.idle_timeout.ms=60000
hadoop.http.logs.enabled=true
hadoop.http.sni.host.check.enabled=false
hadoop.http.staticuser.user=dr.who
hadoop.jetty.logs.serve.aliases=true
hadoop.kerberos.keytab.login.autorenewal.enabled=false
hadoop.kerberos.kinit.command=kinit
hadoop.kerberos.min.seconds.before.relogin=60
hadoop.metrics.jvm.use-thread-mxbean=false
hadoop.prometheus.endpoint.enabled=false
hadoop.registry.jaas.context=Client
hadoop.registry.secure=false
hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@
hadoop.registry.zk.connection.timeout.ms=15000
hadoop.registry.zk.quorum=localhost:2181
hadoop.registry.zk.retry.ceiling.ms=60000
hadoop.registry.zk.retry.interval.ms=1000
hadoop.registry.zk.retry.times=5
hadoop.registry.zk.root=/registry
hadoop.registry.zk.session.timeout.ms=60000
hadoop.rpc.protection=authentication
hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
hadoop.security.auth_to_local.mechanism=hadoop
hadoop.security.authentication=simple
hadoop.security.authorization=false
hadoop.security.credential.clear-text-fallback=true
hadoop.security.crypto.buffer.size=8192
hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding
hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec
hadoop.security.dns.log-slow-lookups.enabled=false
hadoop.security.dns.log-slow-lookups.threshold.ms=1000
hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
hadoop.security.group.mapping.ldap.connection.timeout.ms=60000
hadoop.security.group.mapping.ldap.conversion.rule=none
hadoop.security.group.mapping.ldap.directory.search.timeout=10000
hadoop.security.group.mapping.ldap.num.attempts=3
hadoop.security.group.mapping.ldap.num.attempts.before.failover=3
hadoop.security.group.mapping.ldap.posix.attr.gid.name=gidNumber
hadoop.security.group.mapping.ldap.posix.attr.uid.name=uidNumber
hadoop.security.group.mapping.ldap.read.timeout.ms=60000
hadoop.security.group.mapping.ldap.search.attr.group.name=cn
hadoop.security.group.mapping.ldap.search.attr.member=member
hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
hadoop.security.group.mapping.ldap.search.group.hierarchy.levels=0
hadoop.security.group.mapping.ldap.ssl=false
hadoop.security.group.mapping.providers.combined=true
hadoop.security.groups.cache.background.reload=false
hadoop.security.groups.cache.background.reload.threads=3
hadoop.security.groups.cache.secs=300
hadoop.security.groups.cache.warn.after.ms=5000
hadoop.security.groups.negative-cache.secs=30
hadoop.security.groups.shell.command.timeout=0s
hadoop.security.instrumentation.requires.admin=false
hadoop.security.java.secure.random.algorithm=SHA1PRNG
hadoop.security.key.default.bitlength=128
hadoop.security.key.default.cipher=AES/CTR/NoPadding
hadoop.security.kms.client.authentication.retry-count=1
hadoop.security.kms.client.encrypted.key.cache.expiry=43200000
hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2
hadoop.security.kms.client.encrypted.key.cache.size=500
hadoop.security.kms.client.failover.sleep.base.millis=100
hadoop.security.kms.client.failover.sleep.max.millis=2000
hadoop.security.kms.client.timeout=60
hadoop.security.random.device.file.path=
    /dev/urandom:
hadoop.security.secure.random.impl=org.apache.hadoop.crypto.random.OpensslSecureRandom
hadoop.security.sensitive-config-keys=
      secret$
      password$
      ssl.keystore.pass$
      fs.s3a.server-side-encryption.key
      fs.s3a.*.server-side-encryption.key
      fs.s3a.secret.key
      fs.s3a.*.secret.key
      fs.s3a.session.key
      fs.s3a.*.session.key
      fs.s3a.session.token
      fs.s3a.*.session.token
      fs.azure.account.key.*
      fs.azure.oauth2.*
      fs.adl.oauth2.*
      credential$
      oauth.*secret
      oauth.*password
      oauth.*token
      hadoop.security.sensitive-config-keys

hadoop.security.uid.cache.secs=14400
hadoop.service.shutdown.timeout=30s
hadoop.shell.missing.defaultFs.warning=false
hadoop.shell.safely.delete.limit.num.files=100
hadoop.ssl.client.conf=ssl-client.xml
hadoop.ssl.enabled=false
hadoop.ssl.enabled.protocols=TLSv1.2
hadoop.ssl.hostname.verifier=DEFAULT
hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
hadoop.ssl.require.client.cert=false
hadoop.ssl.server.conf=ssl-server.xml
hadoop.system.tags=YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT
      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL
hadoop.tags.system=YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT
      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL
hadoop.tmp.dir=/tmp/hadoop-Elad.Gazit
hadoop.user.group.static.mapping.overrides=dr.who=;
hadoop.util.hash.type=murmur
hadoop.workaround.non.threadsafe.getpwuid=true
hadoop.zk.acl=world:anyone:rwcda
hadoop.zk.num-retries=1000
hadoop.zk.retry-interval-ms=1000
hadoop.zk.timeout-ms=10000
hive.allow.udf.load.on.demand=false
hive.analyze.stmt.collect.partlevel.stats=true
hive.archive.enabled=false
hive.async.log.enabled=true
hive.ats.hook.queue.capacity=64
hive.auto.convert.join=true
hive.auto.convert.join.hashtable.max.entries=40000000
hive.auto.convert.join.noconditionaltask=true
hive.auto.convert.join.noconditionaltask.size=10000000
hive.auto.convert.join.use.nonstaged=false
hive.auto.convert.sortmerge.join=false
hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ
hive.auto.convert.sortmerge.join.reduce.side=true
hive.auto.convert.sortmerge.join.to.mapjoin=false
hive.auto.progress.timeout=0
hive.autogen.columnalias.prefix.includefuncname=false
hive.autogen.columnalias.prefix.label=_c
hive.binary.record.max.length=1000
hive.blobstore.optimizations.enabled=true
hive.blobstore.supported.schemes=s3,s3a,s3n
hive.blobstore.use.blobstore.as.scratchdir=false
hive.cache.expr.evaluation=true
hive.cbo.cnf.maxnodes=-1
hive.cbo.costmodel.cpu=0.000001
hive.cbo.costmodel.extended=false
hive.cbo.costmodel.hdfs.read=1.5
hive.cbo.costmodel.hdfs.write=10.0
hive.cbo.costmodel.local.fs.read=4.0
hive.cbo.costmodel.local.fs.write=4.0
hive.cbo.costmodel.network=150.0
hive.cbo.enable=false
hive.cbo.returnpath.hiveop=
    false:
hive.cbo.show.warnings=true
hive.cli.errors.ignore=false
hive.cli.pretty.output.num.cols=-1
hive.cli.print.current.db=false
hive.cli.print.header=false
hive.cli.prompt=hive
hive.cli.tez.session.async=true
hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore
hive.cluster.delegation.token.store.zookeeper.znode=/hivedelegation
hive.compactor.abortedtxn.threshold=1000
hive.compactor.check.interval=300
hive.compactor.cleaner.run.interval=5000
hive.compactor.delta.num.threshold=10
hive.compactor.delta.pct.threshold=0.1
hive.compactor.history.reaper.interval=2m
hive.compactor.history.retention.attempted=2
hive.compactor.history.retention.failed=3
hive.compactor.history.retention.succeeded=3
hive.compactor.initiator.failed.compacts.threshold=2
hive.compactor.initiator.on=false
hive.compactor.max.num.delta=500
hive.compactor.worker.threads=0
hive.compactor.worker.timeout=86400
hive.compat=0.12
hive.compute.query.using.stats=true
hive.compute.splits.in.am=true
hive.conf.hidden.list=javax.jdo.option.ConnectionPassword,hive.server2.keystore.password,fs.s3.awsAccessKeyId,fs.s3.awsSecretAccessKey,fs.s3n.awsAccessKeyId,fs.s3n.awsSecretAccessKey,fs.s3a.access.key,fs.s3a.secret.key,fs.s3a.proxy.password
hive.conf.internal.variable.list=hive.added.files.path,hive.added.jars.path,hive.added.archives.path
hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.security.metastore.authorization.manager,hive.security.metastore.authenticator.manager,hive.users.in.admin.role,hive.server2.xsrf.filter.enabled,hive.security.authorization.enabled,hive.server2.authentication.ldap.baseDN,hive.server2.authentication.ldap.url,hive.server2.authentication.ldap.Domain,hive.server2.authentication.ldap.groupDNPattern,hive.server2.authentication.ldap.groupFilter,hive.server2.authentication.ldap.userDNPattern,hive.server2.authentication.ldap.userFilter,hive.server2.authentication.ldap.groupMembershipKey,hive.server2.authentication.ldap.userMembershipKey,hive.server2.authentication.ldap.groupClassKey,hive.server2.authentication.ldap.customLDAPQuery
hive.conf.validation=true
hive.convert.join.bucket.mapjoin.tez=false
hive.count.open.txns.interval=1s
hive.counters.group.name=HIVE
hive.debug.localtask=false
hive.decode.partition.name=false
hive.default.fileformat=TextFile
hive.default.fileformat.managed=none
hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe
hive.default.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.direct.sql.max.elements.in.clause=1000
hive.direct.sql.max.elements.values.clause=1000
hive.direct.sql.max.query.length=100
hive.display.partition.cols.separately=true
hive.downloaded.resources.dir=/var/folders/nw/3qtbss3x3nb2l_1wv2jltw1r0000gq/T/8eabf4ce-ed47-42ae-87d8-5675a699c536_resources
hive.driver.parallel.compilation=false
hive.druid.broker.address.default=localhost:8082
hive.druid.coordinator.address.default=localhost:8081
hive.druid.http.numConnection=20
hive.druid.http.read.timeout=PT1M
hive.druid.indexer.memory.rownum.max=75000
hive.druid.indexer.partition.size.max=5000000
hive.druid.indexer.segments.granularity=DAY
hive.druid.maxTries=5
hive.druid.metadata.base=druid
hive.druid.metadata.db.type=mysql
hive.druid.passiveWaitTimeMs=30000
hive.druid.select.distribute=true
hive.druid.select.threshold=10000
hive.druid.sleep.time=PT10S
hive.druid.storage.storageDirectory=/druid/segments
hive.druid.working.directory=/tmp/workingDirectory
hive.enforce.bucketmapjoin=false
hive.enforce.sortmergebucketmapjoin=false
hive.entity.capture.transform=false
hive.entity.separator=@
hive.error.on.empty.partition=false
hive.exec.check.crossproducts=true
hive.exec.compress.intermediate=false
hive.exec.compress.output=false
hive.exec.concatenate.check.index=true
hive.exec.copyfile.maxnumfiles=1
hive.exec.copyfile.maxsize=33554432
hive.exec.counters.pull.interval=1000
hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__
hive.exec.drop.ignorenonexistent=true
hive.exec.dynamic.partition=true
hive.exec.dynamic.partition.mode=nonstrict
hive.exec.infer.bucket.sort=false
hive.exec.infer.bucket.sort.num.buckets.power.two=false
hive.exec.input.listing.max.threads=0
hive.exec.job.debug.capture.stacktraces=true
hive.exec.job.debug.timeout=30000
hive.exec.local.scratchdir=/var/folders/nw/3qtbss3x3nb2l_1wv2jltw1r0000gq/T/Elad.Gazit
hive.exec.max.created.files=100000
hive.exec.max.dynamic.partitions=1000
hive.exec.max.dynamic.partitions.pernode=100
hive.exec.mode.local.auto=false
hive.exec.mode.local.auto.input.files.max=4
hive.exec.mode.local.auto.inputbytes.max=134217728
hive.exec.orc.base.delta.ratio=8
hive.exec.orc.split.strategy=HYBRID
hive.exec.parallel=false
hive.exec.parallel.thread.number=8
hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger
hive.exec.rcfile.use.explicit.header=true
hive.exec.rcfile.use.sync.cache=true
hive.exec.reducers.bytes.per.reducer=256000000
hive.exec.reducers.max=1009
hive.exec.rowoffset=false
hive.exec.schema.evolution=true
hive.exec.scratchdir=/tmp/hive
hive.exec.script.allow.partial.consumption=false
hive.exec.script.maxerrsize=100000
hive.exec.script.trust=false
hive.exec.show.job.failure.debug.info=true
hive.exec.stagingdir=.hive-staging
hive.exec.submit.local.task.via.child=true
hive.exec.submitviachild=false
hive.exec.tasklog.debug.timeout=20000
hive.exec.temporary.table.storage=default
hive.execution.engine=mr
hive.execution.mode=container
hive.exim.strict.repl.tables=true
hive.exim.uri.scheme.whitelist=hdfs,pfile,file,s3,s3a
hive.explain.dependency.append.tasktype=false
hive.explain.user=true
hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe
hive.fetch.task.aggr=false
hive.fetch.task.conversion=more
hive.fetch.task.conversion.threshold=1073741824
hive.file.max.footer=100
hive.fileformat.check=true
hive.groupby.limit.extrastep=true
hive.groupby.mapaggr.checkinterval=100000
hive.groupby.orderby.position.alias=false
hive.groupby.position.alias=false
hive.groupby.skewindata=false
hive.hash.table.inflation.factor=2.0
hive.hashtable.initialCapacity=100000
hive.hashtable.key.count.adjustment=1.0
hive.hashtable.loadfactor=0.75
hive.hbase.generatehfiles=false
hive.hbase.snapshot.restoredir=/tmp
hive.hbase.wal.enabled=true
hive.heartbeat.interval=1000
hive.hmshandler.force.reload.conf=false
hive.hmshandler.retry.attempts=10
hive.hmshandler.retry.interval=2000
hive.ignore.mapjoin.hint=true
hive.in.test=false
hive.in.tez.test=false
hive.index.compact.binary.search=true
hive.index.compact.file.ignore.hdfs=false
hive.index.compact.query.max.entries=10000000
hive.index.compact.query.max.size=10737418240
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
hive.insert.into.external.tables=true
hive.insert.into.multilevel.dirs=false
hive.int.timestamp.conversion.in.seconds=false
hive.io.rcfile.column.number.conf=0
hive.io.rcfile.record.buffer.size=4194304
hive.io.rcfile.record.interval=2147483647
hive.io.rcfile.tolerate.corruptions=false
hive.io.sarg.cache.max.weight.mb=10
hive.jobname.length=50
hive.join.cache.size=25000
hive.join.emit.interval=1000
hive.lazysimple.extended_boolean_literal=false
hive.limit.optimize.enable=false
hive.limit.optimize.fetch.max=50000
hive.limit.optimize.limit.file=10
hive.limit.pushdown.memory.usage=0.1
hive.limit.query.max.table.partition=-1
hive.limit.row.max.size=100000
hive.llap.allow.permanent.fns=true
hive.llap.am.liveness.connection.sleep.between.retries.ms=2000ms
hive.llap.am.liveness.connection.timeout.ms=10000ms
hive.llap.am.use.fqdn=false
hive.llap.auto.allow.uber=false
hive.llap.auto.auth=false
hive.llap.auto.enforce.stats=true
hive.llap.auto.enforce.tree=true
hive.llap.auto.enforce.vectorized=true
hive.llap.auto.max.input.size=10737418240
hive.llap.auto.max.output.size=1073741824
hive.llap.cache.allow.synthetic.fileid=false
hive.llap.client.consistent.splits=false
hive.llap.daemon.acl=*
hive.llap.daemon.am-reporter.max.threads=4
hive.llap.daemon.am.liveness.heartbeat.interval.ms=10000ms
hive.llap.daemon.communicator.num.threads=10
hive.llap.daemon.delegation.token.lifetime=14d
hive.llap.daemon.download.permanent.fns=false
hive.llap.daemon.logger=query-routing
hive.llap.daemon.memory.per.instance.mb=4096
hive.llap.daemon.num.executors=4
hive.llap.daemon.num.file.cleaner.threads=1
hive.llap.daemon.output.service.max.pending.writes=8
hive.llap.daemon.output.service.port=15003
hive.llap.daemon.output.service.send.buffer.size=131072
hive.llap.daemon.output.stream.timeout=120s
hive.llap.daemon.rpc.num.handlers=5
hive.llap.daemon.rpc.port=0
hive.llap.daemon.service.refresh.interval.sec=60s
hive.llap.daemon.shuffle.dir.watcher.enabled=false
hive.llap.daemon.task.preemption.metrics.intervals=30,60,300
hive.llap.daemon.task.scheduler.enable.preemption=true
hive.llap.daemon.task.scheduler.wait.queue.size=10
hive.llap.daemon.vcpus.per.instance=4
hive.llap.daemon.wait.queue.comparator.class.name=org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator
hive.llap.daemon.web.port=15002
hive.llap.daemon.web.ssl=false
hive.llap.daemon.xmx.headroom=5%
hive.llap.daemon.yarn.container.mb=-1
hive.llap.daemon.yarn.shuffle.port=15551
hive.llap.enable.grace.join.in.llap=false
hive.llap.execution.mode=none
hive.llap.file.cleanup.delay.seconds=300s
hive.llap.hs2.coordinator.enabled=true
hive.llap.io.allocator.alloc.max=16Mb
hive.llap.io.allocator.alloc.min=256Kb
hive.llap.io.allocator.arena.count=8
hive.llap.io.allocator.direct=true
hive.llap.io.allocator.mmap=false
hive.llap.io.allocator.mmap.path=
    /tmp:
hive.llap.io.decoding.metrics.percentiles.intervals=30
hive.llap.io.encode.alloc.size=256Kb
hive.llap.io.encode.enabled=true
hive.llap.io.encode.formats=org.apache.hadoop.mapred.TextInputFormat,
hive.llap.io.encode.slice.lrr=true
hive.llap.io.encode.slice.row.count=100000
hive.llap.io.encode.vector.serde.async.enabled=true
hive.llap.io.encode.vector.serde.enabled=true
hive.llap.io.lrfu.lambda=0.01
hive.llap.io.memory.mode=cache
hive.llap.io.memory.size=1Gb
hive.llap.io.metadata.fraction=0.1
hive.llap.io.nonvector.wrapper.enabled=true
hive.llap.io.orc.time.counters=true
hive.llap.io.threadpool.size=10
hive.llap.io.use.fileid.path=
    true:
hive.llap.io.use.lrfu=true
hive.llap.management.acl=*
hive.llap.management.rpc.port=15004
hive.llap.object.cache.enabled=true
hive.llap.orc.gap.cache=true
hive.llap.remote.token.requires.signing=true
hive.llap.skip.compile.udf.check=false
hive.llap.task.communicator.connection.sleep.between.retries.ms=2000ms
hive.llap.task.communicator.connection.timeout.ms=16000ms
hive.llap.task.communicator.listener.thread-count=30
hive.llap.task.scheduler.locality.delay=0ms
hive.llap.task.scheduler.node.disable.backoff.factor=1.5
hive.llap.task.scheduler.node.reenable.max.timeout.ms=10000ms
hive.llap.task.scheduler.node.reenable.min.timeout.ms=200ms
hive.llap.task.scheduler.num.schedulable.tasks.per.node=0
hive.llap.task.scheduler.timeout.seconds=60s
hive.llap.validate.acls=true
hive.load.dynamic.partitions.thread=15
hive.localize.resource.num.wait.attempts=5
hive.localize.resource.wait.interval=5000
hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
hive.lock.mapred.only.operation=false
hive.lock.numretries=100
hive.lock.sleep.between.retries=60
hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__
hive.log.every.n.records=0
hive.log.explain.output=false
hive.map.aggr=true
hive.map.aggr.hash.force.flush.memory.threshold=0.9
hive.map.aggr.hash.min.reduction=0.5
hive.map.aggr.hash.percentmemory=0.5
hive.map.groupby.sorted=true
hive.mapjoin.bucket.cache.size=100
hive.mapjoin.check.memory.rows=100000
hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55
hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3
hive.mapjoin.hybridgrace.bloomfilter=true
hive.mapjoin.hybridgrace.hashtable=true
hive.mapjoin.hybridgrace.memcheckfrequency=1024
hive.mapjoin.hybridgrace.minnumpartitions=16
hive.mapjoin.hybridgrace.minwbsize=524288
hive.mapjoin.localtask.max.memory.usage=0.9
hive.mapjoin.optimized.hashtable=true
hive.mapjoin.optimized.hashtable.probe.percent=0.5
hive.mapjoin.optimized.hashtable.wbsize=8388608
hive.mapjoin.smalltable.filesize=25000000
hive.mapper.cannot.span.multiple.partitions=false
hive.mapred.local.mem=0
hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner
hive.mapred.reduce.tasks.speculative.execution=true
hive.materializedview.fileformat=ORC
hive.materializedview.rewriting=false
hive.materializedview.serde=org.apache.hadoop.hive.ql.io.orc.OrcSerde
hive.max.open.txns=100000
hive.merge.cardinality.check=true
hive.merge.mapfiles=true
hive.merge.mapredfiles=false
hive.merge.nway.joins=true
hive.merge.orcfile.stripe.level=true
hive.merge.rcfile.block.level=true
hive.merge.size.per.task=256000000
hive.merge.smallfiles.avgsize=16000000
hive.merge.sparkfiles=false
hive.merge.tezfiles=false
hive.metadata.move.exported.metadata.to.trash=true
hive.metastore.aggregate.stats.cache.clean.until=0.8
hive.metastore.aggregate.stats.cache.enabled=true
hive.metastore.aggregate.stats.cache.fpp=0.01
hive.metastore.aggregate.stats.cache.max.full=0.9
hive.metastore.aggregate.stats.cache.max.partitions=10000
hive.metastore.aggregate.stats.cache.max.reader.wait=1000
hive.metastore.aggregate.stats.cache.max.variance=0.01
hive.metastore.aggregate.stats.cache.max.writer.wait=5000
hive.metastore.aggregate.stats.cache.size=10000
hive.metastore.aggregate.stats.cache.ttl=600
hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED
hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED
hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL
hive.metastore.authorization.storage.check.externaltable.drop=true
hive.metastore.authorization.storage.checks=false
hive.metastore.batch.retrieve.max=300
hive.metastore.batch.retrieve.table.partition.max=1000
hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order
hive.metastore.client.capability.check=false
hive.metastore.client.connect.retry.delay=1
hive.metastore.client.drop.partitions.using.expressions=true
hive.metastore.client.socket.lifetime=0
hive.metastore.client.socket.timeout=600
hive.metastore.connect.retries=3
hive.metastore.direct.sql.batch.size=0
hive.metastore.disallow.incompatible.col.type.changes=true
hive.metastore.dml.events=false
hive.metastore.event.clean.freq=0
hive.metastore.event.db.listener.timetolive=86400
hive.metastore.event.expiry.duration=0
hive.metastore.event.message.factory=org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory
hive.metastore.execute.setugi=true
hive.metastore.expression.proxy=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore
hive.metastore.failure.retries=1
hive.metastore.fastpath=
    false:
hive.metastore.filter.hook=org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl
hive.metastore.fshandler.threads=15
hive.metastore.hbase.aggr.stats.cache.entries=10000
hive.metastore.hbase.aggr.stats.hbase.ttl=604800s
hive.metastore.hbase.aggr.stats.invalidator.frequency=5s
hive.metastore.hbase.aggr.stats.memory.ttl=60s
hive.metastore.hbase.aggregate.stats.cache.size=10000
hive.metastore.hbase.aggregate.stats.false.positive.probability=0.01
hive.metastore.hbase.aggregate.stats.max.partitions=10000
hive.metastore.hbase.aggregate.stats.max.variance=0.1
hive.metastore.hbase.cache.clean.until=0.8
hive.metastore.hbase.cache.max.full=0.9
hive.metastore.hbase.cache.max.reader.wait=1000ms
hive.metastore.hbase.cache.max.writer.wait=5000ms
hive.metastore.hbase.cache.ttl=600s
hive.metastore.hbase.catalog.cache.size=50000
hive.metastore.hbase.connection.class=org.apache.hadoop.hive.metastore.hbase.VanillaHBaseConnection
hive.metastore.hbase.file.metadata.threads=1
hive.metastore.initial.metadata.count.enabled=true
hive.metastore.integral.jdo.pushdown=false
hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM
hive.metastore.limit.partition.request=-1
hive.metastore.metrics.enabled=false
hive.metastore.orm.retrieveMapNullsAsEmptyStrings=false
hive.metastore.port=9083
hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
hive.metastore.sasl.enabled=false
hive.metastore.schema.verification=false
hive.metastore.schema.verification.record.version=false
hive.metastore.server.max.message.size=104857600
hive.metastore.server.max.threads=1000
hive.metastore.server.min.threads=200
hive.metastore.server.tcp.keepalive=true
hive.metastore.stats.ndv.densityfunction=false
hive.metastore.stats.ndv.tuner=0.0
hive.metastore.thrift.compact.protocol.enabled=false
hive.metastore.thrift.framed.transport.enabled=false
hive.metastore.try.direct.sql=true
hive.metastore.try.direct.sql.ddl=true
hive.metastore.txn.store.impl=org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler
hive.metastore.use.SSL=false
hive.metastore.warehouse.dir=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse
hive.msck.path.validation=
    throw:
hive.msck.repair.batch.size=0
hive.multi.insert.move.tasks.share.dependencies=false
hive.multigroupby.singlereducer=true
hive.mv.files.thread=15
hive.new.job.grouping.set.cardinality=30
hive.optimize.bucketingsorting=true
hive.optimize.bucketmapjoin=false
hive.optimize.bucketmapjoin.sortedmerge=false
hive.optimize.constant.propagation=true
hive.optimize.correlation=false
hive.optimize.cte.materialize.threshold=-1
hive.optimize.distinct.rewrite=true
hive.optimize.dynamic.partition.hashjoin=false
hive.optimize.filter.stats.reduction=false
hive.optimize.groupby=true
hive.optimize.index.autoupdate=false
hive.optimize.index.filter=false
hive.optimize.index.filter.compact.maxsize=-1
hive.optimize.index.filter.compact.minsize=5368709120
hive.optimize.index.groupby=false
hive.optimize.limittranspose=false
hive.optimize.limittranspose.reductionpercentage=1.0
hive.optimize.limittranspose.reductiontuples=0
hive.optimize.listbucketing=false
hive.optimize.metadataonly=false
hive.optimize.null.scan=true
hive.optimize.partition.columns.separate=true
hive.optimize.point.lookup=true
hive.optimize.point.lookup.min=31
hive.optimize.ppd=true
hive.optimize.ppd.storage=true
hive.optimize.ppd.windowing=true
hive.optimize.reducededuplication=true
hive.optimize.reducededuplication.min.reducer=4
hive.optimize.remove.identity.project=true
hive.optimize.sampling.orderby=false
hive.optimize.sampling.orderby.number=1000
hive.optimize.sampling.orderby.percent=0.1
hive.optimize.semijoin.conversion=true
hive.optimize.skewjoin=false
hive.optimize.skewjoin.compiletime=false
hive.optimize.sort.dynamic.partition=false
hive.optimize.union.remove=false
hive.orc.cache.stripe.details.mem.size=256Mb
hive.orc.cache.use.soft.references=false
hive.orc.compute.splits.num.threads=10
hive.orc.splits.allow.synthetic.fileid=true
hive.orc.splits.directory.batch.ms=0
hive.orc.splits.include.file.footer=false
hive.orc.splits.include.fileid=true
hive.orc.splits.ms.footer.cache.enabled=false
hive.orc.splits.ms.footer.cache.ppd.enabled=true
hive.order.columnalignment=true
hive.orderby.position.alias=true
hive.parquet.timestamp.skip.conversion=true
hive.ppd.recognizetransivity=true
hive.ppd.remove.duplicatefilters=true
hive.prewarm.enabled=false
hive.prewarm.numcontainers=10
hive.query.result.fileformat=SequenceFile
hive.query.timeout.seconds=0s
hive.querylog.enable.plan.progress=true
hive.querylog.location=/var/folders/nw/3qtbss3x3nb2l_1wv2jltw1r0000gq/T/Elad.Gazit
hive.querylog.plan.progress.interval=60000
hive.reorder.nway.joins=true
hive.repl.cm.enabled=false
hive.repl.cm.interval=3600s
hive.repl.cm.retain=24h
hive.repl.cmrootdir=/user/hive/cmroot/
hive.repl.rootdir=/user/hive/repl/
hive.repl.task.factory=org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory
hive.resultset.use.unique.column.names=true
hive.rework.mapredwork=false
hive.rpc.query.plan=false
hive.sample.seednumber=0
hive.scratch.dir.permission=700
hive.scratchdir.lock=false
hive.script.auto.progress=false
hive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist
hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID
hive.script.operator.truncate.env=false
hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader
hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter
hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator
hive.security.authorization.enabled=false
hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory
hive.security.authorization.sqlstd.confwhitelist=hive\.auto\..*|hive\.cbo\..*|hive\.convert\..*|hive\.exec\.dynamic\.partition.*|hive\.exec\.max\.dynamic\.partitions.*|hive\.exec\.compress\..*|hive\.exec\.infer\..*|hive\.exec\.mode.local\..*|hive\.exec\.orc\..*|hive\.exec\.parallel.*|hive\.explain\..*|hive\.fetch.task\..*|hive\.groupby\..*|hive\.hbase\..*|hive\.index\..*|hive\.index\..*|hive\.intermediate\..*|hive\.join\..*|hive\.limit\..*|hive\.log\..*|hive\.mapjoin\..*|hive\.merge\..*|hive\.optimize\..*|hive\.orc\..*|hive\.outerjoin\..*|hive\.parquet\..*|hive\.ppd\..*|hive\.prewarm\..*|hive\.server2\.thrift\.resultset\.default\.fetch\.size|hive\.server2\.proxy\.user|hive\.skewjoin\..*|hive\.smbjoin\..*|hive\.stats\..*|hive\.strict\..*|hive\.tez\..*|hive\.vectorized\..*|mapred\.map\..*|mapred\.reduce\..*|mapred\.output\.compression\.codec|mapred\.job\.queuename|mapred\.output\.compression\.type|mapred\.min\.split\.size|mapreduce\.job\.reduce\.slowstart\.completedmaps|mapreduce\.job\.queuename|mapreduce\.job\.tags|mapreduce\.input\.fileinputformat\.split\.minsize|mapreduce\.map\..*|mapreduce\.reduce\..*|mapreduce\.output\.fileoutputformat\.compress\.codec|mapreduce\.output\.fileoutputformat\.compress\.type|oozie\..*|tez\.am\..*|tez\.task\..*|tez\.runtime\..*|tez\.queue\.name|hive\.transpose\.aggr\.join|hive\.exec\.reducers\.bytes\.per\.reducer|hive\.client\.stats\.counters|hive\.exec\.default\.partition\.name|hive\.exec\.drop\.ignorenonexistent|hive\.counters\.group\.name|hive\.default\.fileformat\.managed|hive\.enforce\.bucketmapjoin|hive\.enforce\.sortmergebucketmapjoin|hive\.cache\.expr\.evaluation|hive\.query\.result\.fileformat|hive\.hashtable\.loadfactor|hive\.hashtable\.initialCapacity|hive\.ignore\.mapjoin\.hint|hive\.limit\.row\.max\.size|hive\.mapred\.mode|hive\.map\.aggr|hive\.compute\.query\.using\.stats|hive\.exec\.rowoffset|hive\.variable\.substitute|hive\.variable\.substitute\.depth|hive\.autogen\.columnalias\.prefix\.includefuncname|hive\.autogen\.columnalias\.prefix\.label|hive\.exec\.check\.crossproducts|hive\.cli\.tez\.session\.async|hive\.compat|hive\.exec\.concatenate\.check\.index|hive\.display\.partition\.cols\.separately|hive\.error\.on\.empty\.partition|hive\.execution\.engine|hive\.exec\.copyfile\.maxsize|hive\.exim\.uri\.scheme\.whitelist|hive\.file\.max\.footer|hive\.insert\.into\.multilevel\.dirs|hive\.localize\.resource\.num\.wait\.attempts|hive\.multi\.insert\.move\.tasks\.share\.dependencies|hive\.support\.quoted\.identifiers|hive\.resultset\.use\.unique\.column\.names|hive\.analyze\.stmt\.collect\.partlevel\.stats|hive\.exec\.schema\.evolution|hive\.server2\.logging\.operation\.level|hive\.server2\.thrift\.resultset\.serialize\.in\.tasks|hive\.support\.special\.characters\.tablename|hive\.exec\.job\.debug\.capture\.stacktraces|hive\.exec\.job\.debug\.timeout|hive\.llap\.io\.enabled|hive\.llap\.io\.use\.fileid\.path|hive\.llap\.daemon\.service\.hosts|hive\.llap\.execution\.mode|hive\.llap\.auto\.allow\.uber|hive\.llap\.auto\.enforce\.tree|hive\.llap\.auto\.enforce\.vectorized|hive\.llap\.auto\.enforce\.stats|hive\.llap\.auto\.max\.input\.size|hive\.llap\.auto\.max\.output\.size|hive\.llap\.skip\.compile\.udf\.check|hive\.llap\.client\.consistent\.splits|hive\.llap\.enable\.grace\.join\.in\.llap|hive\.llap\.allow\.permanent\.fns|hive\.exec\.max\.created\.files|hive\.exec\.reducers\.max|hive\.reorder\.nway\.joins|hive\.output\.file\.extension|hive\.exec\.show\.job\.failure\.debug\.info|hive\.exec\.tasklog\.debug\.timeout|hive\.query\.id
hive.security.authorization.task.factory=org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl
hive.security.command.whitelist=set,reset,dfs,add,list,delete,reload,compile
hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
hive.security.metastore.authorization.auth.reads=true
hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider
hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
hive.server.read.socket.timeout=10
hive.server.tcp.keepalive=true
hive.server2.allow.user.substitution=true
hive.server2.async.exec.async.compile=false
hive.server2.async.exec.keepalive.time=10
hive.server2.async.exec.shutdown.timeout=10
hive.server2.async.exec.threads=100
hive.server2.async.exec.wait.queue.size=100
hive.server2.authentication=NONE
hive.server2.authentication.ldap.groupClassKey=groupOfNames
hive.server2.authentication.ldap.groupMembershipKey=member
hive.server2.authentication.ldap.guidKey=uid
hive.server2.clear.dangling.scratchdir=false
hive.server2.clear.dangling.scratchdir.interval=1800s
hive.server2.close.session.on.disconnect=true
hive.server2.compile.lock.timeout=0s
hive.server2.enable.doAs=true
hive.server2.global.init.file.location=${env:HIVE_CONF_DIR}
hive.server2.idle.operation.timeout=432000000
hive.server2.idle.session.check.operation=true
hive.server2.idle.session.timeout=604800000
hive.server2.in.place.progress=true
hive.server2.llap.concurrent.queries=-1
hive.server2.logging.operation.enabled=true
hive.server2.logging.operation.level=EXECUTION
hive.server2.logging.operation.log.location=/var/folders/nw/3qtbss3x3nb2l_1wv2jltw1r0000gq/T/Elad.Gazit/operation_logs
hive.server2.long.polling.timeout=5000
hive.server2.map.fair.scheduler.queue=true
hive.server2.max.start.attempts=30
hive.server2.metrics.enabled=false
hive.server2.parallel.ops.in.session=true
hive.server2.session.check.interval=21600000
hive.server2.sleep.interval.between.start.attempts=60s
hive.server2.support.dynamic.service.discovery=false
hive.server2.table.type.mapping=CLASSIC
hive.server2.tez.initialize.default.sessions=false
hive.server2.tez.session.lifetime=162h
hive.server2.tez.session.lifetime.jitter=3h
hive.server2.tez.sessions.custom.queue.allowed=true
hive.server2.tez.sessions.init.threads=16
hive.server2.tez.sessions.per.default.queue=1
hive.server2.thrift.client.connect.retry.limit=1
hive.server2.thrift.client.password=anonymous
hive.server2.thrift.client.retry.delay.seconds=1s
hive.server2.thrift.client.retry.limit=1
hive.server2.thrift.client.user=anonymous
hive.server2.thrift.exponential.backoff.slot.length=100
hive.server2.thrift.http.cookie.auth.enabled=true
hive.server2.thrift.http.cookie.is.httponly=true
hive.server2.thrift.http.cookie.is.secure=true
hive.server2.thrift.http.cookie.max.age=86400
hive.server2.thrift.http.max.idle.time=1800000
hive.server2.thrift.http.path=
    cliservice:
hive.server2.thrift.http.port=10001
hive.server2.thrift.http.request.header.size=6144
hive.server2.thrift.http.response.header.size=6144
hive.server2.thrift.http.worker.keepalive.time=60
hive.server2.thrift.login.timeout=20
hive.server2.thrift.max.message.size=104857600
hive.server2.thrift.max.worker.threads=500
hive.server2.thrift.min.worker.threads=5
hive.server2.thrift.port=10000
hive.server2.thrift.resultset.default.fetch.size=1000
hive.server2.thrift.resultset.max.fetch.size=10000
hive.server2.thrift.resultset.serialize.in.tasks=false
hive.server2.thrift.sasl.qop=auth
hive.server2.thrift.worker.keepalive.time=60
hive.server2.transport.mode=binary
hive.server2.use.SSL=false
hive.server2.webui.host=0.0.0.0
hive.server2.webui.max.historic.queries=25
hive.server2.webui.max.threads=50
hive.server2.webui.port=10002
hive.server2.webui.spnego.principal=HTTP/_HOST@EXAMPLE.COM
hive.server2.webui.use.spnego=false
hive.server2.webui.use.ssl=false
hive.server2.xsrf.filter.enabled=false
hive.server2.zookeeper.namespace=hiveserver2
hive.server2.zookeeper.publish.configs=true
hive.service.metrics.class=org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics
hive.service.metrics.file.frequency=5s
hive.service.metrics.file.location=/tmp/report.json
hive.service.metrics.hadoop2.component=hive
hive.service.metrics.hadoop2.frequency=30s
hive.service.metrics.reporter=JSON_FILE, JMX
hive.session.history.enabled=false
hive.session.id=8eabf4ce-ed47-42ae-87d8-5675a699c536
hive.session.silent=false
hive.skewjoin.key=100000
hive.skewjoin.mapjoin.map.tasks=10000
hive.skewjoin.mapjoin.min.split=33554432
hive.smbjoin.cache.rows=10000
hive.spark.client.connect.timeout=1000
hive.spark.client.future.timeout=60
hive.spark.client.rpc.max.size=52428800
hive.spark.client.rpc.sasl.mechanisms=DIGEST-MD5
hive.spark.client.rpc.threads=8
hive.spark.client.secret.bits=256
hive.spark.client.server.connect.timeout=90000
hive.spark.dynamic.partition.pruning=false
hive.spark.dynamic.partition.pruning.max.data.size=104857600
hive.spark.exec.inplace.progress=true
hive.spark.job.monitor.timeout=60
hive.spark.use.file.size.for.mapjoin=false
hive.spark.use.groupby.shuffle=true
hive.spark.use.op.stats=true
hive.ssl.protocol.blacklist=SSLv2,SSLv3
hive.stageid.rearrange=none
hive.start.cleanup.scratchdir=false
hive.stats.atomic=false
hive.stats.autogather=true
hive.stats.collect.scancols=false
hive.stats.collect.tablekeys=false
hive.stats.column.autogather=false
hive.stats.dbclass=fs
hive.stats.deserialization.factor=1.0
hive.stats.fetch.column.stats=false
hive.stats.fetch.partition.stats=true
hive.stats.filter.in.factor=1.0
hive.stats.gather.num.threads=10
hive.stats.jdbc.timeout=30
hive.stats.join.factor=1.1
hive.stats.list.num.entries=10
hive.stats.map.num.entries=10
hive.stats.max.variable.length=100
hive.stats.ndv.error=20.0
hive.stats.reliable=false
hive.stats.retries.wait=3000
hive.strict.checks.bucketing=true
hive.strict.checks.cartesian.product=true
hive.strict.checks.large.query=false
hive.strict.checks.type.safety=true
hive.support.concurrency=false
hive.support.quoted.identifiers=column
hive.support.special.characters.tablename=true
hive.test.authz.sstd.hs2.mode=false
hive.test.fail.compaction=false
hive.test.fail.heartbeater=false
hive.test.mode=false
hive.test.mode.prefix=test_
hive.test.mode.samplefreq=32
hive.test.rollbacktxn=false
hive.tez.auto.reducer.parallelism=false
hive.tez.bigtable.minsize.semijoin.reduction=1000000
hive.tez.bloom.filter.factor=2.0
hive.tez.bucket.pruning=false
hive.tez.bucket.pruning.compat=true
hive.tez.container.max.java.heap.fraction=0.8
hive.tez.container.size=-1
hive.tez.cpu.vcores=-1
hive.tez.dynamic.partition.pruning=true
hive.tez.dynamic.partition.pruning.max.data.size=104857600
hive.tez.dynamic.partition.pruning.max.event.size=1048576
hive.tez.dynamic.semijoin.reduction=true
hive.tez.dynamic.semijoin.reduction.threshold=0.5
hive.tez.enable.memory.manager=true
hive.tez.exec.inplace.progress=true
hive.tez.exec.print.summary=false
hive.tez.hs2.user.access=true
hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat
hive.tez.input.generate.consistent.splits=true
hive.tez.log.level=INFO
hive.tez.max.bloom.filter.entries=100000000
hive.tez.max.partition.factor=2.0
hive.tez.min.bloom.filter.entries=1000000
hive.tez.min.partition.factor=0.25
hive.tez.smb.number.waves=0.5
hive.tez.task.scale.memory.reserve-fraction.min=0.3
hive.tez.task.scale.memory.reserve.fraction=-1.0
hive.tez.task.scale.memory.reserve.fraction.max=0.5
hive.timedout.txn.reaper.interval=180s
hive.timedout.txn.reaper.start=100s
hive.transactional.events.mem=10000000
hive.transactional.table.scan=false
hive.transform.escape.input=false
hive.transpose.aggr.join=false
hive.txn.heartbeat.threadpool.size=5
hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager
hive.txn.manager.dump.lock.state.on.acquire.timeout=false
hive.txn.max.open.batch=1000
hive.txn.operational.properties=0
hive.txn.strict.locking.mode=true
hive.txn.timeout=300
hive.typecheck.on.insert=true
hive.udtf.auto.progress=false
hive.unlock.numretries=10
hive.user.install.directory=/user/
hive.variable.substitute=true
hive.variable.substitute.depth=40
hive.vectorized.adaptor.usage.mode=all
hive.vectorized.execution.enabled=false
hive.vectorized.execution.mapjoin.minmax.enabled=false
hive.vectorized.execution.mapjoin.native.enabled=true
hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false
hive.vectorized.execution.mapjoin.native.multikey.only.enabled=false
hive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1
hive.vectorized.execution.reduce.enabled=true
hive.vectorized.execution.reduce.groupby.enabled=true
hive.vectorized.execution.reducesink.new.enabled=true
hive.vectorized.groupby.checkinterval=100000
hive.vectorized.groupby.flush.percent=0.1
hive.vectorized.groupby.maxentries=1000000
hive.vectorized.use.row.serde.deserialize=false
hive.vectorized.use.vector.serde.deserialize=true
hive.vectorized.use.vectorized.input.format=true
hive.warehouse.subdir.inherit.perms=true
hive.writeset.reaper.interval=60s
hive.zookeeper.clean.extra.nodes=false
hive.zookeeper.client.port=2181
hive.zookeeper.connection.basesleeptime=1000
hive.zookeeper.connection.max.retries=3
hive.zookeeper.namespace=hive_zookeeper_namespace
hive.zookeeper.session.timeout=1200000
io.bytes.per.checksum=512
io.compression.codec.bzip2.library=system-native
io.erasurecode.codec.rs-legacy.rawcoders=rs-legacy_java
io.erasurecode.codec.rs.rawcoders=rs_native,rs_java
io.erasurecode.codec.xor.rawcoders=xor_native,xor_java
io.file.buffer.size=65536
io.map.index.interval=128
io.map.index.skip=0
io.mapfile.bloom.error.rate=0.005
io.mapfile.bloom.size=1048576
io.seqfile.compress.blocksize=1000000
io.seqfile.local.dir=/tmp/hadoop-Elad.Gazit/io/local
io.serializations=org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
io.skip.checksum.errors=false
ipc.[port_number].backoff.enable=false
ipc.[port_number].callqueue.impl=java.util.concurrent.LinkedBlockingQueue
ipc.[port_number].cost-provider.impl=org.apache.hadoop.ipc.DefaultCostProvider
ipc.[port_number].decay-scheduler.backoff.responsetime.enable=false
ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds=10s,20s,30s,40s
ipc.[port_number].decay-scheduler.decay-factor=0.5
ipc.[port_number].decay-scheduler.metrics.top.user.count=10
ipc.[port_number].decay-scheduler.period-ms=5000
ipc.[port_number].decay-scheduler.thresholds=13,25,50
ipc.[port_number].faircallqueue.multiplexer.weights=8,4,2,1
ipc.[port_number].identity-provider.impl=org.apache.hadoop.ipc.UserIdentityProvider
ipc.[port_number].scheduler.impl=org.apache.hadoop.ipc.DefaultRpcScheduler
ipc.[port_number].scheduler.priority.levels=4
ipc.[port_number].weighted-cost.handler=1
ipc.[port_number].weighted-cost.lockexclusive=100
ipc.[port_number].weighted-cost.lockfree=1
ipc.[port_number].weighted-cost.lockshared=10
ipc.[port_number].weighted-cost.response=1
ipc.client.bind.wildcard.addr=false
ipc.client.connect.max.retries=10
ipc.client.connect.max.retries.on.timeouts=45
ipc.client.connect.retry.interval=1000
ipc.client.connect.timeout=20000
ipc.client.connection.maxidletime=10000
ipc.client.fallback-to-simple-auth-allowed=false
ipc.client.idlethreshold=4000
ipc.client.kill.max=10
ipc.client.low-latency=false
ipc.client.ping=true
ipc.client.rpc-timeout.ms=0
ipc.client.tcpnodelay=true
ipc.maximum.data.length=134217728
ipc.maximum.response.length=134217728
ipc.ping.interval=60000
ipc.server.listen.queue.size=256
ipc.server.log.slow.rpc=false
ipc.server.max.connections=0
ipc.server.reuseaddr=true
javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver
javax.jdo.option.ConnectionPassword=
javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=metastore_db;create=true
javax.jdo.option.ConnectionUserName=APP
javax.jdo.option.DetachAllOnCommit=true
javax.jdo.option.Multithreaded=true
javax.jdo.option.NonTransactionalRead=true
map.sort.class=org.apache.hadoop.util.QuickSort
mapreduce.am.max-attempts=2
mapreduce.app-submission.cross-platform=false
mapreduce.client.completion.pollinterval=5000
mapreduce.client.libjars.wildcard=true
mapreduce.client.output.filter=FAILED
mapreduce.client.progressmonitor.pollinterval=1000
mapreduce.client.submit.file.replication=10
mapreduce.cluster.acls.enabled=false
mapreduce.cluster.local.dir=/tmp/hadoop-Elad.Gazit/mapred/local
mapreduce.fileoutputcommitter.algorithm.version=1
mapreduce.fileoutputcommitter.task.cleanup.enabled=false
mapreduce.framework.name=local
mapreduce.ifile.readahead=true
mapreduce.ifile.readahead.bytes=4194304
mapreduce.input.fileinputformat.list-status.num-threads=1
mapreduce.input.fileinputformat.split.maxsize=256000000
mapreduce.input.fileinputformat.split.minsize=0
mapreduce.input.fileinputformat.split.minsize.per.node=1
mapreduce.input.fileinputformat.split.minsize.per.rack=1
mapreduce.input.lineinputformat.linespermap=1
mapreduce.job.acl-modify-job=
mapreduce.job.acl-view-job=
mapreduce.job.cache.limit.max-resources=0
mapreduce.job.cache.limit.max-resources-mb=0
mapreduce.job.cache.limit.max-single-resource-mb=0
mapreduce.job.classloader=false
mapreduce.job.committer.setup.cleanup.needed=true
mapreduce.job.complete.cancel.delegation.tokens=true
mapreduce.job.counters.max=120
mapreduce.job.dfs.storage.capacity.kill-limit-exceed=false
mapreduce.job.emit-timeline-data=false
mapreduce.job.encrypted-intermediate-data=false
mapreduce.job.encrypted-intermediate-data-key-size-bits=128
mapreduce.job.encrypted-intermediate-data.buffer.kb=128
mapreduce.job.end-notification.max.attempts=5
mapreduce.job.end-notification.max.retry.interval=5000
mapreduce.job.end-notification.retry.attempts=0
mapreduce.job.end-notification.retry.interval=1000
mapreduce.job.finish-when-all-reducers-done=true
mapreduce.job.hdfs-servers=file:///
mapreduce.job.heap.memory-mb.ratio=0.8
mapreduce.job.local-fs.single-disk-limit.bytes=-1
mapreduce.job.local-fs.single-disk-limit.check.interval-ms=5000
mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed=true
mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer
mapreduce.job.maps=2
mapreduce.job.max.map=-1
mapreduce.job.max.split.locations=15
mapreduce.job.maxtaskfailures.per.tracker=3
mapreduce.job.queuename=default
mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle
mapreduce.job.reduce.slowstart.completedmaps=0.05
mapreduce.job.reducer.preempt.delay.sec=0
mapreduce.job.reducer.unconditional-preempt.delay.sec=300
mapreduce.job.reduces=1
mapreduce.job.running.map.limit=0
mapreduce.job.running.reduce.limit=0
mapreduce.job.sharedcache.mode=disabled
mapreduce.job.speculative.minimum-allowed-tasks=10
mapreduce.job.speculative.retry-after-no-speculate=1000
mapreduce.job.speculative.retry-after-speculate=15000
mapreduce.job.speculative.slowtaskthreshold=1.0
mapreduce.job.speculative.speculative-cap-running-tasks=0.1
mapreduce.job.speculative.speculative-cap-total-tasks=0.01
mapreduce.job.split.metainfo.maxsize=10000000
mapreduce.job.token.tracking.ids.enabled=false
mapreduce.job.ubertask.enable=false
mapreduce.job.ubertask.maxmaps=9
mapreduce.job.ubertask.maxreduces=1
mapreduce.jobhistory.address=0.0.0.0:10020
mapreduce.jobhistory.admin.acl=*
mapreduce.jobhistory.admin.address=0.0.0.0:10033
mapreduce.jobhistory.always-scan-user-dir=false
mapreduce.jobhistory.cleaner.enable=true
mapreduce.jobhistory.cleaner.interval-ms=86400000
mapreduce.jobhistory.client.thread-count=10
mapreduce.jobhistory.datestring.cache.size=200000
mapreduce.jobhistory.done-dir=/tmp/hadoop-yarn/staging/history/done
mapreduce.jobhistory.http.policy=HTTP_ONLY
mapreduce.jobhistory.intermediate-done-dir=/tmp/hadoop-yarn/staging/history/done_intermediate
mapreduce.jobhistory.intermediate-user-done-dir.permissions=770
mapreduce.jobhistory.jhist.format=binary
mapreduce.jobhistory.joblist.cache.size=20000
mapreduce.jobhistory.jobname.limit=50
mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab
mapreduce.jobhistory.loadedjob.tasks.max=-1
mapreduce.jobhistory.loadedjobs.cache.size=5
mapreduce.jobhistory.max-age-ms=604800000
mapreduce.jobhistory.minicluster.fixed.ports=false
mapreduce.jobhistory.move.interval-ms=180000
mapreduce.jobhistory.move.thread-count=3
mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD
mapreduce.jobhistory.recovery.enable=false
mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService
mapreduce.jobhistory.recovery.store.fs.uri=/tmp/hadoop-Elad.Gazit/mapred/history/recoverystore
mapreduce.jobhistory.recovery.store.leveldb.path=
    /tmp/hadoop-Elad.Gazit/mapred/history/recoverystore:
mapreduce.jobhistory.webapp.address=0.0.0.0:19888
mapreduce.jobhistory.webapp.https.address=0.0.0.0:19890
mapreduce.jobhistory.webapp.rest-csrf.custom-header=X-XSRF-Header
mapreduce.jobhistory.webapp.rest-csrf.enabled=false
mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
mapreduce.jobhistory.webapp.xfs-filter.xframe-options=SAMEORIGIN
mapreduce.jvm.system-properties-to-log=os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name
mapreduce.map.cpu.vcores=1
mapreduce.map.log.level=INFO
mapreduce.map.maxattempts=4
mapreduce.map.memory.mb=-1
mapreduce.map.output.compress=false
mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
mapreduce.map.skip.maxrecords=0
mapreduce.map.skip.proc-count.auto-incr=true
mapreduce.map.sort.spill.percent=0.80
mapreduce.map.speculative=true
mapreduce.output.fileoutputformat.compress=false
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
mapreduce.output.fileoutputformat.compress.type=RECORD
mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory
mapreduce.reduce.cpu.vcores=1
mapreduce.reduce.input.buffer.percent=0.0
mapreduce.reduce.log.level=INFO
mapreduce.reduce.markreset.buffer.percent=0.0
mapreduce.reduce.maxattempts=4
mapreduce.reduce.memory.mb=-1
mapreduce.reduce.merge.inmem.threshold=1000
mapreduce.reduce.shuffle.connect.timeout=180000
mapreduce.reduce.shuffle.fetch.retry.enabled=false
mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000
mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000
mapreduce.reduce.shuffle.input.buffer.percent=0.70
mapreduce.reduce.shuffle.memory.limit.percent=0.25
mapreduce.reduce.shuffle.merge.percent=0.66
mapreduce.reduce.shuffle.parallelcopies=5
mapreduce.reduce.shuffle.read.timeout=180000
mapreduce.reduce.shuffle.retry-delay.max.ms=60000
mapreduce.reduce.skip.maxgroups=0
mapreduce.reduce.skip.proc-count.auto-incr=true
mapreduce.reduce.speculative=true
mapreduce.shuffle.connection-keep-alive.enable=false
mapreduce.shuffle.connection-keep-alive.timeout=5
mapreduce.shuffle.listen.queue.size=128
mapreduce.shuffle.max.connections=0
mapreduce.shuffle.max.threads=0
mapreduce.shuffle.pathcache.concurrency-level=
    16:
mapreduce.shuffle.pathcache.expire-after-access-minutes=
    5:
mapreduce.shuffle.pathcache.max-weight=
    10485760:
mapreduce.shuffle.port=13562
mapreduce.shuffle.ssl.enabled=false
mapreduce.shuffle.ssl.file.buffer.size=65536
mapreduce.shuffle.transfer.buffer.size=131072
mapreduce.task.combine.progress.records=10000
mapreduce.task.exit.timeout=60000
mapreduce.task.exit.timeout.check-interval-ms=20000
mapreduce.task.files.preserve.failedtasks=false
mapreduce.task.io.sort.factor=10
mapreduce.task.io.sort.mb=100
mapreduce.task.local-fs.write-limit.bytes=-1
mapreduce.task.merge.progress.records=10000
mapreduce.task.profile=false
mapreduce.task.profile.map.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.maps=0-2
mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.reduce.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.reduces=0-2
mapreduce.task.skip.start.attempts=2
mapreduce.task.stuck.timeout-ms=600000
mapreduce.task.timeout=600000
mapreduce.task.userlog.limit.kb=0
net.topology.impl=org.apache.hadoop.net.NetworkTopology
net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
net.topology.script.number.args=100
nfs.exports.allowed.hosts=* rw
parquet.memory.pool.ratio=0.5
rpc.metrics.quantile.enable=false
seq.io.sort.factor=100
seq.io.sort.mb=100
spark.app.id=local-1645260654119
spark.app.name=9b2953c3-5cd9-4441-a865-27cf9d6569e6
spark.app.startTime=1645260653292
spark.driver.host=192.168.1.101
spark.driver.port=52504
spark.executor.id=driver
spark.master=local[*]
spark.sql.catalogImplementation=hive
spark.sql.warehouse.dir=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse
stream.stderr.reporter.enabled=true
stream.stderr.reporter.prefix=reporter:
tfile.fs.input.buffer.size=262144
tfile.fs.output.buffer.size=262144
tfile.io.chunk.size=1048576
yarn.acl.enable=false
yarn.acl.reservation-enable=false
yarn.admin.acl=*
yarn.am.liveness-monitor.expiry-interval-ms=600000
yarn.app.attempt.diagnostics.limit.kc=64
yarn.app.mapreduce.am.command-opts=-Xmx1024m
yarn.app.mapreduce.am.container.log.backups=0
yarn.app.mapreduce.am.container.log.limit.kb=0
yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10
yarn.app.mapreduce.am.hard-kill-timeout-ms=10000
yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
yarn.app.mapreduce.am.job.committer.commit-window=10000
yarn.app.mapreduce.am.job.task.listener.thread-count=30
yarn.app.mapreduce.am.log.level=INFO
yarn.app.mapreduce.am.resource.cpu-vcores=1
yarn.app.mapreduce.am.resource.mb=1536
yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled=false
yarn.app.mapreduce.am.webapp.https.client.auth=false
yarn.app.mapreduce.am.webapp.https.enabled=false
yarn.app.mapreduce.client-am.ipc.max-retries=3
yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3
yarn.app.mapreduce.client.job.max-retries=3
yarn.app.mapreduce.client.job.retry-interval=2000
yarn.app.mapreduce.client.max-retries=3
yarn.app.mapreduce.shuffle.log.backups=0
yarn.app.mapreduce.shuffle.log.limit.kb=0
yarn.app.mapreduce.shuffle.log.separate=true
yarn.app.mapreduce.task.container.log.backups=0
yarn.bin.path=
    yarn:
yarn.client.application-client-protocol.poll-interval-ms=200
yarn.client.application-client-protocol.poll-timeout-ms=-1
yarn.client.failover-no-ha-proxy-provider=org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider
yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider
yarn.client.failover-retries=0
yarn.client.failover-retries-on-socket-timeouts=0
yarn.client.load.resource-types.from-server=false
yarn.client.max-cached-nodemanagers-proxies=0
yarn.client.nodemanager-client-async.thread-pool-max-size=500
yarn.client.nodemanager-connect.max-wait-ms=180000
yarn.client.nodemanager-connect.retry-interval-ms=10000
yarn.cluster.max-application-priority=0
yarn.dispatcher.cpu-monitor.samples-per-min=60
yarn.dispatcher.drain-events.timeout=300000
yarn.dispatcher.print-events-info.threshold=5000
yarn.fail-fast=false
yarn.federation.cache-ttl.secs=300
yarn.federation.enabled=false
yarn.federation.registry.base-dir=yarnfederation/
yarn.federation.state-store.class=org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore
yarn.federation.subcluster-resolver.class=org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl
yarn.http.policy=HTTP_ONLY
yarn.intermediate-data-encryption.enable=false
yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
yarn.is.minicluster=false
yarn.log-aggregation-enable=false
yarn.log-aggregation-status.time-out.ms=600000
yarn.log-aggregation.debug.filesize=104857600
yarn.log-aggregation.file-controller.TFile.class=org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController
yarn.log-aggregation.file-formats=TFile
yarn.log-aggregation.retain-check-interval-seconds=-1
yarn.log-aggregation.retain-seconds=-1
yarn.minicluster.control-resource-monitoring=false
yarn.minicluster.fixed.ports=false
yarn.minicluster.use-rpc=false
yarn.minicluster.yarn.nodemanager.resource.memory-mb=4096
yarn.nm.liveness-monitor.expiry-interval-ms=600000
yarn.node-attribute.fs-store.impl.class=org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore
yarn.node-labels.configuration-type=centralized
yarn.node-labels.enabled=false
yarn.node-labels.fs-store.impl.class=org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore
yarn.nodemanager.address=0.0.0.0:0
yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
yarn.nodemanager.amrmproxy.address=0.0.0.0:8049
yarn.nodemanager.amrmproxy.client.thread-count=25
yarn.nodemanager.amrmproxy.enabled=false
yarn.nodemanager.amrmproxy.ha.enable=false
yarn.nodemanager.amrmproxy.interceptor-class.pipeline=org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor
yarn.nodemanager.aux-services.manifest.enabled=false
yarn.nodemanager.aux-services.manifest.reload-ms=0
yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
yarn.nodemanager.collector-service.address=0.0.0.0:8048
yarn.nodemanager.collector-service.thread-count=5
yarn.nodemanager.container-diagnostics-maximum-size=10000
yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
yarn.nodemanager.container-executor.exit-code-file.timeout-ms=2000
yarn.nodemanager.container-localizer.java.opts=-Xmx256m
yarn.nodemanager.container-localizer.log.level=INFO
yarn.nodemanager.container-log-monitor.dir-size-limit-bytes=1000000000
yarn.nodemanager.container-log-monitor.enable=false
yarn.nodemanager.container-log-monitor.interval-ms=60000
yarn.nodemanager.container-log-monitor.total-size-limit-bytes=10000000000
yarn.nodemanager.container-manager.thread-count=20
yarn.nodemanager.container-metrics.enable=true
yarn.nodemanager.container-metrics.period-ms=-1
yarn.nodemanager.container-metrics.unregister-delay-ms=10000
yarn.nodemanager.container-monitor.enabled=true
yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false
yarn.nodemanager.container-retry-minimum-interval-ms=1000
yarn.nodemanager.container.stderr.pattern={*stderr*,*STDERR*}
yarn.nodemanager.container.stderr.tail.bytes=4096
yarn.nodemanager.containers-launcher.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
yarn.nodemanager.default-container-executor.log-dirs.permissions=710
yarn.nodemanager.delete.debug-delay-sec=0
yarn.nodemanager.delete.thread-count=4
yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled=true
yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled=true
yarn.nodemanager.disk-health-checker.enable=true
yarn.nodemanager.disk-health-checker.interval-ms=120000
yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb=0
yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
yarn.nodemanager.disk-validator=basic
yarn.nodemanager.distributed-scheduling.enabled=false
yarn.nodemanager.elastic-memory-control.enabled=false
yarn.nodemanager.elastic-memory-control.oom-handler=org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler
yarn.nodemanager.elastic-memory-control.timeout-sec=5
yarn.nodemanager.emit-container-events=true
yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ
yarn.nodemanager.health-checker.interval-ms=600000
yarn.nodemanager.health-checker.run-before-startup=false
yarn.nodemanager.health-checker.scripts=script
yarn.nodemanager.health-checker.timeout-ms=1200000
yarn.nodemanager.hostname=0.0.0.0
yarn.nodemanager.keytab=/etc/krb5.keytab
yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms=20
yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms=1000
yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
yarn.nodemanager.linux-container-executor.cgroups.mount=false
yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false
yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true
yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody
yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$
yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
yarn.nodemanager.local-cache.max-files-per-directory=8192
yarn.nodemanager.local-dirs=/tmp/hadoop-Elad.Gazit/nm-local-dir
yarn.nodemanager.localizer.address=0.0.0.0:8040
yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
yarn.nodemanager.localizer.cache.target-size-mb=10240
yarn.nodemanager.localizer.client.thread-count=5
yarn.nodemanager.localizer.fetch.thread-count=4
yarn.nodemanager.log-aggregation.compression-type=none
yarn.nodemanager.log-aggregation.num-log-files-per-app=30
yarn.nodemanager.log-aggregation.policy.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min=3600
yarn.nodemanager.log-container-debug-info.enabled=true
yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
yarn.nodemanager.log.deletion-threads-count=4
yarn.nodemanager.log.retain-seconds=10800
yarn.nodemanager.logaggregation.threadpool-size-max=100
yarn.nodemanager.node-attributes.provider.fetch-interval-ms=600000
yarn.nodemanager.node-attributes.provider.fetch-timeout-ms=1200000
yarn.nodemanager.node-attributes.resync-interval-ms=120000
yarn.nodemanager.node-labels.provider.fetch-interval-ms=600000
yarn.nodemanager.node-labels.provider.fetch-timeout-ms=1200000
yarn.nodemanager.node-labels.resync-interval-ms=120000
yarn.nodemanager.numa-awareness.enabled=false
yarn.nodemanager.numa-awareness.numactl.cmd=/usr/bin/numactl
yarn.nodemanager.numa-awareness.read-topology=false
yarn.nodemanager.opportunistic-containers-max-queue-length=0
yarn.nodemanager.opportunistic-containers-use-pause-for-preemption=false
yarn.nodemanager.pluggable-device-framework.enabled=false
yarn.nodemanager.pmem-check-enabled=true
yarn.nodemanager.process-kill-wait.ms=5000
yarn.nodemanager.recovery.compaction-interval-secs=3600
yarn.nodemanager.recovery.dir=/tmp/hadoop-Elad.Gazit/yarn-nm-recovery
yarn.nodemanager.recovery.enabled=false
yarn.nodemanager.recovery.supervised=false
yarn.nodemanager.remote-app-log-dir=/tmp/logs
yarn.nodemanager.remote-app-log-dir-include-older=true
yarn.nodemanager.remote-app-log-dir-suffix=logs
yarn.nodemanager.resource-monitor.interval-ms=3000
yarn.nodemanager.resource-plugins.fpga.allowed-fpga-devices=auto
yarn.nodemanager.resource-plugins.fpga.vendor-plugin.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin
yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices=auto
yarn.nodemanager.resource-plugins.gpu.docker-plugin=nvidia-docker-v1
yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidia-docker-v1.endpoint=http://localhost:3476/v1.0/docker/cli
yarn.nodemanager.resource.count-logical-processors-as-cores=false
yarn.nodemanager.resource.cpu-vcores=-1
yarn.nodemanager.resource.detect-hardware-capabilities=false
yarn.nodemanager.resource.memory-mb=-1
yarn.nodemanager.resource.memory.cgroups.soft-limit-percentage=90.0
yarn.nodemanager.resource.memory.cgroups.swappiness=0
yarn.nodemanager.resource.memory.enabled=false
yarn.nodemanager.resource.memory.enforced=true
yarn.nodemanager.resource.pcores-vcores-multiplier=1.0
yarn.nodemanager.resource.percentage-physical-cpu-limit=100
yarn.nodemanager.resource.system-reserved-memory-mb=-1
yarn.nodemanager.resourcemanager.minimum.version=NONE
yarn.nodemanager.runtime.linux.allowed-runtimes=default
yarn.nodemanager.runtime.linux.docker.allowed-container-networks=host,none,bridge
yarn.nodemanager.runtime.linux.docker.allowed-container-runtimes=runc
yarn.nodemanager.runtime.linux.docker.capabilities=CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE
yarn.nodemanager.runtime.linux.docker.default-container-network=host
yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed=false
yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed=true
yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed=false
yarn.nodemanager.runtime.linux.docker.image-update=false
yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed=false
yarn.nodemanager.runtime.linux.docker.stop.grace-period=10
yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold=1
yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold=1
yarn.nodemanager.runtime.linux.runc.allowed-container-networks=host,none,bridge
yarn.nodemanager.runtime.linux.runc.allowed-container-runtimes=runc
yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size=500
yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs=360
yarn.nodemanager.runtime.linux.runc.host-pid-namespace.allowed=false
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin=org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs=60
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.hdfs-hash-file=/runc-root/image-tag-to-hash
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache=10
yarn.nodemanager.runtime.linux.runc.image-toplevel-dir=/runc-root
yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs=600
yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep=100
yarn.nodemanager.runtime.linux.runc.manifest-to-resources-plugin=org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin
yarn.nodemanager.runtime.linux.runc.privileged-containers.allowed=false
yarn.nodemanager.runtime.linux.sandbox-mode=disabled
yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions=read
yarn.nodemanager.sleep-delay-before-sigkill.ms=250
yarn.nodemanager.vmem-check-enabled=true
yarn.nodemanager.vmem-pmem-ratio=2.1
yarn.nodemanager.webapp.address=0.0.0.0:8042
yarn.nodemanager.webapp.cross-origin.enabled=false
yarn.nodemanager.webapp.https.address=0.0.0.0:8044
yarn.nodemanager.webapp.rest-csrf.custom-header=X-XSRF-Header
yarn.nodemanager.webapp.rest-csrf.enabled=false
yarn.nodemanager.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
yarn.nodemanager.webapp.xfs-filter.xframe-options=SAMEORIGIN
yarn.nodemanager.windows-container.cpu-limit.enabled=false
yarn.nodemanager.windows-container.memory-limit.enabled=false
yarn.registry.class=org.apache.hadoop.registry.client.impl.FSRegistryOperationsService
yarn.resourcemanager.activities-manager.app-activities.max-queue-length=100
yarn.resourcemanager.activities-manager.app-activities.ttl-ms=600000
yarn.resourcemanager.activities-manager.cleanup-interval-ms=5000
yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms=600000
yarn.resourcemanager.address=0.0.0.0:8032
yarn.resourcemanager.admin.address=0.0.0.0:8033
yarn.resourcemanager.admin.client.thread-count=1
yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.am.max-attempts=2
yarn.resourcemanager.amlauncher.thread-count=50
yarn.resourcemanager.application-https.policy=NONE
yarn.resourcemanager.application-tag-based-placement.enable=false
yarn.resourcemanager.application-timeouts.monitor.interval-ms=3000
yarn.resourcemanager.application.max-tag.length=100
yarn.resourcemanager.application.max-tags=10
yarn.resourcemanager.auto-update.containers=false
yarn.resourcemanager.client.thread-count=50
yarn.resourcemanager.configuration.file-system-based-store=/yarn/conf
yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider
yarn.resourcemanager.connect.max-wait.ms=900000
yarn.resourcemanager.connect.retry-interval.ms=30000
yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs=20
yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
yarn.resourcemanager.delegation-token-renewer.thread-count=50
yarn.resourcemanager.delegation-token-renewer.thread-retry-interval=60s
yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts=10
yarn.resourcemanager.delegation-token-renewer.thread-timeout=60s
yarn.resourcemanager.delegation-token.always-cancel=false
yarn.resourcemanager.delegation-token.max-conf-size-bytes=12800
yarn.resourcemanager.delegation.key.update-interval=86400000
yarn.resourcemanager.delegation.token.max-lifetime=604800000
yarn.resourcemanager.delegation.token.renew-interval=86400000
yarn.resourcemanager.epoch.range=0
yarn.resourcemanager.fail-fast=false
yarn.resourcemanager.fs.state-store.num-retries=0
yarn.resourcemanager.fs.state-store.retry-interval-ms=1000
yarn.resourcemanager.fs.state-store.uri=/tmp/hadoop-Elad.Gazit/yarn/system/rmstore
yarn.resourcemanager.ha.automatic-failover.embedded=true
yarn.resourcemanager.ha.automatic-failover.enabled=true
yarn.resourcemanager.ha.automatic-failover.zk-base-path=
    /yarn-leader-election:
yarn.resourcemanager.ha.enabled=false
yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size=10
yarn.resourcemanager.hostname=0.0.0.0
yarn.resourcemanager.keytab=/etc/krb5.keytab
yarn.resourcemanager.leveldb-state-store.compaction-interval-secs=3600
yarn.resourcemanager.leveldb-state-store.path=
    /tmp/hadoop-Elad.Gazit/yarn/system/rmstore:
yarn.resourcemanager.max-completed-applications=1000
yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10
yarn.resourcemanager.metrics.runtime.buckets=60,300,1440
yarn.resourcemanager.nm-container-queuing.load-comparator=QUEUE_LENGTH
yarn.resourcemanager.nm-container-queuing.max-queue-length=15
yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms=100
yarn.resourcemanager.nm-container-queuing.min-queue-length=5
yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms=10
yarn.resourcemanager.nm-container-queuing.queue-limit-stdev=1.0f
yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms=1000
yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.node-ip-cache.expiry-interval-secs=-1
yarn.resourcemanager.node-labels.provider.fetch-interval-ms=1800000
yarn.resourcemanager.node-removal-untracked.timeout-ms=60000
yarn.resourcemanager.nodemanager-connect-retries=10
yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs=3600
yarn.resourcemanager.nodemanager.minimum.version=NONE
yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms=1000
yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms=1000
yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
yarn.resourcemanager.nodemanagers.heartbeat-interval-scaling-enable=false
yarn.resourcemanager.nodemanagers.heartbeat-interval-slowdown-factor=1.0
yarn.resourcemanager.nodemanagers.heartbeat-interval-speedup-factor=1.0
yarn.resourcemanager.opportunistic-container-allocation.enabled=false
yarn.resourcemanager.opportunistic-container-allocation.nodes-used=10
yarn.resourcemanager.opportunistic.max.container-allocation.per.am.heartbeat=-1
yarn.resourcemanager.placement-constraints.algorithm.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm
yarn.resourcemanager.placement-constraints.algorithm.iterator=SERIAL
yarn.resourcemanager.placement-constraints.algorithm.pool-size=1
yarn.resourcemanager.placement-constraints.handler=disabled
yarn.resourcemanager.placement-constraints.retry-attempts=3
yarn.resourcemanager.placement-constraints.scheduler.pool-size=1
yarn.resourcemanager.proxy-user-privileges.enabled=false
yarn.resourcemanager.recovery.enabled=false
yarn.resourcemanager.reservation-system.enable=false
yarn.resourcemanager.reservation-system.planfollower.time-step=1000
yarn.resourcemanager.resource-profiles.enabled=false
yarn.resourcemanager.resource-profiles.source-file=resource-profiles.json
yarn.resourcemanager.resource-tracker.address=0.0.0.0:8031
yarn.resourcemanager.resource-tracker.client.thread-count=50
yarn.resourcemanager.resource-tracker.nm.ip-hostname-check=false
yarn.resourcemanager.rm.container-allocation.expiry-interval-ms=600000
yarn.resourcemanager.scheduler.address=0.0.0.0:8030
yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
yarn.resourcemanager.scheduler.client.thread-count=50
yarn.resourcemanager.scheduler.monitor.enable=false
yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
yarn.resourcemanager.state-store.max-completed-applications=1000
yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
yarn.resourcemanager.submission-preprocessor.enabled=false
yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms=60000
yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10
yarn.resourcemanager.system-metrics-publisher.enabled=false
yarn.resourcemanager.webapp.address=0.0.0.0:8088
yarn.resourcemanager.webapp.cross-origin.enabled=false
yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true
yarn.resourcemanager.webapp.https.address=0.0.0.0:8090
yarn.resourcemanager.webapp.rest-csrf.custom-header=X-XSRF-Header
yarn.resourcemanager.webapp.rest-csrf.enabled=false
yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
yarn.resourcemanager.webapp.ui-actions.enabled=true
yarn.resourcemanager.webapp.xfs-filter.xframe-options=SAMEORIGIN
yarn.resourcemanager.work-preserving-recovery.enabled=true
yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000
yarn.resourcemanager.zk-appid-node.split-index=0
yarn.resourcemanager.zk-delegation-token-node.split-index=0
yarn.resourcemanager.zk-max-znode-size.bytes=1048576
yarn.resourcemanager.zk-state-store.parent-path=
    /rmstore:
yarn.rm.system-metrics-publisher.emit-container-events=false
yarn.router.clientrm.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor
yarn.router.interceptor.user.threadpool-size=5
yarn.router.pipeline.cache-max-size=25
yarn.router.rmadmin.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor
yarn.router.webapp.address=0.0.0.0:8089
yarn.router.webapp.https.address=0.0.0.0:8091
yarn.router.webapp.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST
yarn.scheduler.configuration.fs.path=
    file:
    ///tmp/hadoop-Elad.Gazit/yarn/system/schedconf:
yarn.scheduler.configuration.leveldb-store.compaction-interval-secs=86400
yarn.scheduler.configuration.leveldb-store.path=
    /tmp/hadoop-Elad.Gazit/yarn/system/confstore:
yarn.scheduler.configuration.max.version=100
yarn.scheduler.configuration.mutation.acl-policy.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy
yarn.scheduler.configuration.store.class=file
yarn.scheduler.configuration.store.max-logs=1000
yarn.scheduler.configuration.zk-store.parent-path=
    /confstore:
yarn.scheduler.include-port-in-node-name=false
yarn.scheduler.maximum-allocation-mb=8192
yarn.scheduler.maximum-allocation-vcores=4
yarn.scheduler.minimum-allocation-mb=1024
yarn.scheduler.minimum-allocation-vcores=1
yarn.scheduler.queue-placement-rules=user-group
yarn.sharedcache.admin.address=0.0.0.0:8047
yarn.sharedcache.admin.thread-count=1
yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker
yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl
yarn.sharedcache.cleaner.initial-delay-mins=10
yarn.sharedcache.cleaner.period-mins=1440
yarn.sharedcache.cleaner.resource-sleep-ms=0
yarn.sharedcache.client-server.address=0.0.0.0:8045
yarn.sharedcache.client-server.thread-count=50
yarn.sharedcache.enabled=false
yarn.sharedcache.nested-level=3
yarn.sharedcache.nm.uploader.replication.factor=10
yarn.sharedcache.nm.uploader.thread-count=20
yarn.sharedcache.root-dir=/sharedcache
yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore
yarn.sharedcache.store.in-memory.check-period-mins=720
yarn.sharedcache.store.in-memory.initial-delay-mins=10
yarn.sharedcache.store.in-memory.staleness-period-mins=10080
yarn.sharedcache.uploader.server.address=0.0.0.0:8046
yarn.sharedcache.uploader.server.thread-count=50
yarn.sharedcache.webapp.address=0.0.0.0:8788
yarn.system-metrics-publisher.enabled=false
yarn.timeline-service.address=0.0.0.0:10200
yarn.timeline-service.app-aggregation-interval-secs=15
yarn.timeline-service.app-collector.linger-period.ms=60000
yarn.timeline-service.client.best-effort=false
yarn.timeline-service.client.drain-entities.timeout.ms=2000
yarn.timeline-service.client.fd-clean-interval-secs=60
yarn.timeline-service.client.fd-flush-interval-secs=10
yarn.timeline-service.client.fd-retain-secs=300
yarn.timeline-service.client.internal-timers-ttl-secs=420
yarn.timeline-service.client.max-retries=30
yarn.timeline-service.client.retry-interval-ms=1000
yarn.timeline-service.enabled=false
yarn.timeline-service.entity-group-fs-store.active-dir=/tmp/entity-file-history/active
yarn.timeline-service.entity-group-fs-store.app-cache-size=10
yarn.timeline-service.entity-group-fs-store.cache-store-class=org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore
yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds=3600
yarn.timeline-service.entity-group-fs-store.done-dir=/tmp/entity-file-history/done/
yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size=10485760
yarn.timeline-service.entity-group-fs-store.retain-seconds=604800
yarn.timeline-service.entity-group-fs-store.scan-interval-seconds=60
yarn.timeline-service.entity-group-fs-store.summary-store=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
yarn.timeline-service.entity-group-fs-store.with-user-dir=false
yarn.timeline-service.flowname.max-size=0
yarn.timeline-service.generic-application-history.max-applications=10000
yarn.timeline-service.handler-thread-count=10
yarn.timeline-service.hbase-schema.prefix=prod.
yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds=259200000
yarn.timeline-service.hbase.coprocessor.jar.hdfs.location=/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar
yarn.timeline-service.hostname=0.0.0.0
yarn.timeline-service.http-authentication.simple.anonymous.allowed=true
yarn.timeline-service.http-authentication.type=simple
yarn.timeline-service.http-cross-origin.enabled=false
yarn.timeline-service.keytab=/etc/krb5.keytab
yarn.timeline-service.leveldb-state-store.path=
    /tmp/hadoop-Elad.Gazit/yarn/timeline:
yarn.timeline-service.leveldb-timeline-store.path=
    /tmp/hadoop-Elad.Gazit/yarn/timeline:
yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600
yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000
yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000
yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000
yarn.timeline-service.reader.class=org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl
yarn.timeline-service.reader.webapp.address=0.0.0.0:8188
yarn.timeline-service.reader.webapp.https.address=0.0.0.0:8190
yarn.timeline-service.recovery.enabled=false
yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore
yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
yarn.timeline-service.timeline-client.number-of-async-entities-to-merge=10
yarn.timeline-service.ttl-enable=true
yarn.timeline-service.ttl-ms=604800000
yarn.timeline-service.version=1.0f
yarn.timeline-service.webapp.address=0.0.0.0:8188
yarn.timeline-service.webapp.https.address=0.0.0.0:8190
yarn.timeline-service.webapp.rest-csrf.custom-header=X-XSRF-Header
yarn.timeline-service.webapp.rest-csrf.enabled=false
yarn.timeline-service.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
yarn.timeline-service.webapp.xfs-filter.xframe-options=SAMEORIGIN
yarn.timeline-service.writer.async.queue.capacity=100
yarn.timeline-service.writer.class=org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl
yarn.timeline-service.writer.flush-interval-seconds=60
yarn.webapp.api-service.enable=false
yarn.webapp.enable-rest-app-submissions=true
yarn.webapp.filter-entity-list-by-user=false
yarn.webapp.filter-invalid-xml-chars=false
yarn.webapp.ui2.enable=false
yarn.webapp.xfs-filter.enabled=true
yarn.workflow-id.tag-prefix=workflowid:
END========"new HiveConf()"========

22/02/19 10:50:59 INFO TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
22/02/19 10:50:59 WARN TxnHandler: Cannot perform cleanup since metastore table does not exist
22/02/19 10:50:59 INFO TableSplitter: create database if not exists mixedtiles
22/02/19 10:50:59 INFO HiveMetaStore: 0: create_database: Database(name:mixedtiles, description:, locationUri:file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db, parameters:{}, ownerName:Elad.Gazit)
22/02/19 10:50:59 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=create_database: Database(name:mixedtiles, description:, locationUri:file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db, parameters:{}, ownerName:Elad.Gazit)
22/02/19 10:50:59 WARN ObjectStore: Failed to get database mixedtiles, returning NoSuchObjectException
22/02/19 10:50:59 INFO FileUtils: Creating directory if it doesn't exist: file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db
22/02/19 10:50:59 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.
22/02/19 10:50:59 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:00 INFO FileSourceStrategy: Pushed Filters:
22/02/19 10:51:00 INFO FileSourceStrategy: Post-Scan Filters:
22/02/19 10:51:00 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
22/02/19 10:51:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 189.8 KiB, free 4.6 GiB)
22/02/19 10:51:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.8 KiB, free 4.6 GiB)
22/02/19 10:51:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.101:52505 (size: 32.8 KiB, free: 4.6 GiB)
22/02/19 10:51:00 INFO SparkContext: Created broadcast 0 from json at TableSplitter.scala:41
22/02/19 10:51:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:00 INFO SparkContext: Starting job: json at TableSplitter.scala:41
22/02/19 10:51:00 INFO DAGScheduler: Got job 0 (json at TableSplitter.scala:41) with 1 output partitions
22/02/19 10:51:00 INFO DAGScheduler: Final stage: ResultStage 0 (json at TableSplitter.scala:41)
22/02/19 10:51:00 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:00 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at TableSplitter.scala:41), which has no missing parents
22/02/19 10:51:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.3 KiB, free 4.6 GiB)
22/02/19 10:51:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 4.6 GiB)
22/02/19 10:51:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.101:52505 (size: 6.3 KiB, free: 4.6 GiB)
22/02/19 10:51:00 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at TableSplitter.scala:41) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
22/02/19 10:51:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes) taskResourceAssignments Map()
22/02/19 10:51:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/02/19 10:51:00 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/src/main/resources/seed, range: 0-1918, partition values: [empty row]
22/02/19 10:51:01 INFO CodeGenerator: Code generated in 182.803062 ms
22/02/19 10:51:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2275 bytes result sent to driver
22/02/19 10:51:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 395 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
22/02/19 10:51:01 INFO DAGScheduler: ResultStage 0 (json at TableSplitter.scala:41) finished in 0.512 s
22/02/19 10:51:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/02/19 10:51:01 INFO DAGScheduler: Job 0 finished: json at TableSplitter.scala:41, took 0.551152 s
22/02/19 10:51:01 INFO FileSourceStrategy: Pushed Filters:
22/02/19 10:51:01 INFO FileSourceStrategy: Post-Scan Filters:
22/02/19 10:51:01 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string>
22/02/19 10:51:01 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
22/02/19 10:51:01 INFO CodeGenerator: Code generated in 34.732785 ms
22/02/19 10:51:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 189.7 KiB, free 4.6 GiB)
22/02/19 10:51:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 4.6 GiB)
22/02/19 10:51:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.101:52505 (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:01 INFO SparkContext: Created broadcast 2 from collect at TableSplitter.scala:18
22/02/19 10:51:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:01 INFO DAGScheduler: Registering RDD 7 (collect at TableSplitter.scala:18) as input to shuffle 0
22/02/19 10:51:01 INFO DAGScheduler: Got map stage job 1 (collect at TableSplitter.scala:18) with 1 output partitions
22/02/19 10:51:01 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at TableSplitter.scala:18)
22/02/19 10:51:01 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:01 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:01 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at collect at TableSplitter.scala:18), which has no missing parents
22/02/19 10:51:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.9 KiB, free 4.6 GiB)
22/02/19 10:51:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 4.6 GiB)
22/02/19 10:51:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.101:52505 (size: 10.5 KiB, free: 4.6 GiB)
22/02/19 10:51:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at collect at TableSplitter.scala:18) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
22/02/19 10:51:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7836 bytes) taskResourceAssignments Map()
22/02/19 10:51:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/02/19 10:51:01 INFO CodeGenerator: Code generated in 9.859451 ms
22/02/19 10:51:01 INFO CodeGenerator: Code generated in 5.708556 ms
22/02/19 10:51:01 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/src/main/resources/seed, range: 0-1918, partition values: [empty row]
22/02/19 10:51:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2809 bytes result sent to driver
22/02/19 10:51:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 141 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
22/02/19 10:51:01 INFO DAGScheduler: ShuffleMapStage 1 (collect at TableSplitter.scala:18) finished in 0.157 s
22/02/19 10:51:01 INFO DAGScheduler: looking for newly runnable stages
22/02/19 10:51:01 INFO DAGScheduler: running: HashSet()
22/02/19 10:51:01 INFO DAGScheduler: waiting: HashSet()
22/02/19 10:51:01 INFO DAGScheduler: failed: HashSet()
22/02/19 10:51:01 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
22/02/19 10:51:01 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
22/02/19 10:51:01 INFO CodeGenerator: Code generated in 16.69042 ms
22/02/19 10:51:01 INFO SparkContext: Starting job: collect at TableSplitter.scala:18
22/02/19 10:51:01 INFO DAGScheduler: Got job 2 (collect at TableSplitter.scala:18) with 1 output partitions
22/02/19 10:51:01 INFO DAGScheduler: Final stage: ResultStage 3 (collect at TableSplitter.scala:18)
22/02/19 10:51:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
22/02/19 10:51:01 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:01 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at collect at TableSplitter.scala:18), which has no missing parents
22/02/19 10:51:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 25.8 KiB, free 4.6 GiB)
22/02/19 10:51:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 4.6 GiB)
22/02/19 10:51:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.101:52505 (size: 12.3 KiB, free: 4.6 GiB)
22/02/19 10:51:01 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at collect at TableSplitter.scala:18) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:01 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
22/02/19 10:51:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (192.168.1.101, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
22/02/19 10:51:01 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
22/02/19 10:51:01 INFO ShuffleBlockFetcherIterator: Getting 1 (408.0 B) non-empty blocks including 1 (408.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
22/02/19 10:51:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
22/02/19 10:51:01 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 3986 bytes result sent to driver
22/02/19 10:51:01 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 63 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
22/02/19 10:51:01 INFO DAGScheduler: ResultStage 3 (collect at TableSplitter.scala:18) finished in 0.073 s
22/02/19 10:51:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
22/02/19 10:51:01 INFO DAGScheduler: Job 2 finished: collect at TableSplitter.scala:18, took 0.084696 s
22/02/19 10:51:01 INFO CodeGenerator: Code generated in 8.480272 ms
22/02/19 10:51:01 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:01 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:01 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:01 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_name),EqualTo(event_name,contact_support)
22/02/19 10:51:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_name#7),(event_name#7 = contact_support)
22/02/19 10:51:01 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:01 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:01 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:01 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:01 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:01 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:01 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:01 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:01 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:01 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:01 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:02 INFO CodeGenerator: Code generated in 12.770911 ms
22/02/19 10:51:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 189.7 KiB, free 4.6 GiB)
22/02/19 10:51:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 4.6 GiB)
22/02/19 10:51:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.101:52505 (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:02 INFO SparkContext: Created broadcast 5 from saveAsTable at TableSplitter.scala:35
22/02/19 10:51:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:02 INFO SparkContext: Starting job: saveAsTable at TableSplitter.scala:35
22/02/19 10:51:02 INFO DAGScheduler: Got job 3 (saveAsTable at TableSplitter.scala:35) with 1 output partitions
22/02/19 10:51:02 INFO DAGScheduler: Final stage: ResultStage 4 (saveAsTable at TableSplitter.scala:35)
22/02/19 10:51:02 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:02 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:02 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at saveAsTable at TableSplitter.scala:35), which has no missing parents
22/02/19 10:51:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 209.6 KiB, free 4.6 GiB)
22/02/19 10:51:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 75.6 KiB, free 4.6 GiB)
22/02/19 10:51:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.101:52505 (size: 75.6 KiB, free: 4.6 GiB)
22/02/19 10:51:02 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at saveAsTable at TableSplitter.scala:35) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:02 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
22/02/19 10:51:02 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes) taskResourceAssignments Map()
22/02/19 10:51:02 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
22/02/19 10:51:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:02 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:02 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:02 INFO ParquetOutputFormat: Parquet block size to 134217728
22/02/19 10:51:02 INFO ParquetOutputFormat: Validation is off
22/02/19 10:51:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/02/19 10:51:02 INFO ParquetOutputFormat: Parquet properties are:
Parquet page size to 1048576
Parquet dictionary page size to 1048576
Dictionary is true
Writer version is: PARQUET_1_0
Page size checking is: estimated
Min row count for page size check is: 100
Max row count for page size check is: 10000
Truncate length for column indexes is: 64
Truncate length for statistics min/max  is: 2147483647
Bloom filter enabled: false
Max Bloom filter size for a column is 1048576
Bloom filter expected number of distinct values are: null
Page row count limit to 20000
Writing page checksums is: on
22/02/19 10:51:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "properties",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_agent",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary event_name (STRING);
  optional binary event_time (STRING);
  optional binary properties (STRING);
  optional binary session_id (STRING);
  optional binary user_agent (STRING);
  optional binary uuid (STRING);
}


22/02/19 10:51:02 INFO CodecPool: Got brand-new compressor [.snappy]
22/02/19 10:51:02 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/src/main/resources/seed, range: 0-1918, partition values: [empty row]
22/02/19 10:51:02 INFO CodeGenerator: Code generated in 7.86132 ms
22/02/19 10:51:02 INFO CodeGenerator: Code generated in 4.703361 ms
22/02/19 10:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_202202191051026060035122364591516_0004_m_000000_3' to file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/contact_support/_temporary/0/task_202202191051026060035122364591516_0004_m_000000
22/02/19 10:51:02 INFO SparkHadoopMapRedUtil: attempt_202202191051026060035122364591516_0004_m_000000_3: Committed
22/02/19 10:51:02 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2872 bytes result sent to driver
22/02/19 10:51:02 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 754 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:02 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
22/02/19 10:51:02 INFO DAGScheduler: ResultStage 4 (saveAsTable at TableSplitter.scala:35) finished in 0.789 s
22/02/19 10:51:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
22/02/19 10:51:02 INFO DAGScheduler: Job 3 finished: saveAsTable at TableSplitter.scala:35, took 0.791610 s
22/02/19 10:51:02 INFO FileFormatWriter: Start to commit write Job 71d5d8bb-7656-4d79-b45d-d52a96135b98.
22/02/19 10:51:02 INFO FileFormatWriter: Write Job 71d5d8bb-7656-4d79-b45d-d52a96135b98 committed. Elapsed time: 16 ms.
22/02/19 10:51:02 INFO FileFormatWriter: Finished processing stats for write job 71d5d8bb-7656-4d79-b45d-d52a96135b98.
22/02/19 10:51:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:02 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:02 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:02 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:02 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:02 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:02 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:02 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:02 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:03 INFO HiveExternalCatalog: Persisting file based data source table `mixedtiles`.`contact_support` into Hive metastore in Hive compatible format.
22/02/19 10:51:03 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=8eabf4ce-ed47-42ae-87d8-5675a699c536, clientType=HIVECLI]
22/02/19 10:51:03 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
22/02/19 10:51:03 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
22/02/19 10:51:03 INFO HiveMetaStore: 0: Cleaning up thread local RawStore...
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...
22/02/19 10:51:03 INFO HiveMetaStore: 0: Done cleaning up thread local RawStore
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore
22/02/19 10:51:03 INFO HiveMetaStore: 0: create_table: Table(tableName:contact_support, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260661, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/contact_support, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=create_table: Table(tableName:contact_support, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260661, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/contact_support, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:03 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
22/02/19 10:51:03 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
22/02/19 10:51:03 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
22/02/19 10:51:03 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
22/02/19 10:51:03 INFO ObjectStore: ObjectStore, initialize called
22/02/19 10:51:03 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
22/02/19 10:51:03 INFO ObjectStore: Initialized ObjectStore
22/02/19 10:51:03 INFO log: Updating table stats fast for contact_support
22/02/19 10:51:03 INFO log: Updated size of table contact_support to 2854
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_name),EqualTo(event_name,signup)
22/02/19 10:51:03 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_name#7),(event_name#7 = signup)
22/02/19 10:51:03 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 189.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.101:52505 (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:03 INFO SparkContext: Created broadcast 7 from saveAsTable at TableSplitter.scala:35
22/02/19 10:51:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:03 INFO SparkContext: Starting job: saveAsTable at TableSplitter.scala:35
22/02/19 10:51:03 INFO DAGScheduler: Got job 4 (saveAsTable at TableSplitter.scala:35) with 1 output partitions
22/02/19 10:51:03 INFO DAGScheduler: Final stage: ResultStage 5 (saveAsTable at TableSplitter.scala:35)
22/02/19 10:51:03 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:03 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at saveAsTable at TableSplitter.scala:35), which has no missing parents
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 209.5 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 75.6 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.101:52505 (size: 75.6 KiB, free: 4.6 GiB)
22/02/19 10:51:03 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at saveAsTable at TableSplitter.scala:35) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
22/02/19 10:51:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes) taskResourceAssignments Map()
22/02/19 10:51:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:03 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
22/02/19 10:51:03 INFO ParquetOutputFormat: Validation is off
22/02/19 10:51:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/02/19 10:51:03 INFO ParquetOutputFormat: Parquet properties are:
Parquet page size to 1048576
Parquet dictionary page size to 1048576
Dictionary is true
Writer version is: PARQUET_1_0
Page size checking is: estimated
Min row count for page size check is: 100
Max row count for page size check is: 10000
Truncate length for column indexes is: 64
Truncate length for statistics min/max  is: 2147483647
Bloom filter enabled: false
Max Bloom filter size for a column is 1048576
Bloom filter expected number of distinct values are: null
Page row count limit to 20000
Writing page checksums is: on
22/02/19 10:51:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "properties",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_agent",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary event_name (STRING);
  optional binary event_time (STRING);
  optional binary properties (STRING);
  optional binary session_id (STRING);
  optional binary user_agent (STRING);
  optional binary uuid (STRING);
}


22/02/19 10:51:03 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/src/main/resources/seed, range: 0-1918, partition values: [empty row]
22/02/19 10:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_202202191051034154146300969886063_0005_m_000000_4' to file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/signup/_temporary/0/task_202202191051034154146300969886063_0005_m_000000
22/02/19 10:51:03 INFO SparkHadoopMapRedUtil: attempt_202202191051034154146300969886063_0005_m_000000_4: Committed
22/02/19 10:51:03 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 2829 bytes result sent to driver
22/02/19 10:51:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 62 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
22/02/19 10:51:03 INFO DAGScheduler: ResultStage 5 (saveAsTable at TableSplitter.scala:35) finished in 0.107 s
22/02/19 10:51:03 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
22/02/19 10:51:03 INFO DAGScheduler: Job 4 finished: saveAsTable at TableSplitter.scala:35, took 0.109412 s
22/02/19 10:51:03 INFO FileFormatWriter: Start to commit write Job 521869b1-d73d-46a8-9626-016eb53c4609.
22/02/19 10:51:03 INFO FileFormatWriter: Write Job 521869b1-d73d-46a8-9626-016eb53c4609 committed. Elapsed time: 15 ms.
22/02/19 10:51:03 INFO FileFormatWriter: Finished processing stats for write job 521869b1-d73d-46a8-9626-016eb53c4609.
22/02/19 10:51:03 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=signup
22/02/19 10:51:03 INFO HiveExternalCatalog: Persisting file based data source table `mixedtiles`.`signup` into Hive metastore in Hive compatible format.
22/02/19 10:51:03 INFO HiveMetaStore: 0: create_table: Table(tableName:signup, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260663, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/signup, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=create_table: Table(tableName:signup, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260663, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/signup, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:03 INFO log: Updating table stats fast for signup
22/02/19 10:51:03 INFO log: Updated size of table signup to 2813
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_name),EqualTo(event_name,event name with spaces)
22/02/19 10:51:03 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_name#7),(event_name#7 = event name with spaces)
22/02/19 10:51:03 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 189.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.101:52505 (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:03 INFO SparkContext: Created broadcast 9 from saveAsTable at TableSplitter.scala:35
22/02/19 10:51:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:03 INFO SparkContext: Starting job: saveAsTable at TableSplitter.scala:35
22/02/19 10:51:03 INFO DAGScheduler: Got job 5 (saveAsTable at TableSplitter.scala:35) with 1 output partitions
22/02/19 10:51:03 INFO DAGScheduler: Final stage: ResultStage 6 (saveAsTable at TableSplitter.scala:35)
22/02/19 10:51:03 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:03 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:03 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at saveAsTable at TableSplitter.scala:35), which has no missing parents
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 209.6 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 75.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.1.101:52505 (size: 75.7 KiB, free: 4.6 GiB)
22/02/19 10:51:03 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at saveAsTable at TableSplitter.scala:35) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:03 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
22/02/19 10:51:03 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes) taskResourceAssignments Map()
22/02/19 10:51:03 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:03 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
22/02/19 10:51:03 INFO ParquetOutputFormat: Validation is off
22/02/19 10:51:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/02/19 10:51:03 INFO ParquetOutputFormat: Parquet properties are:
Parquet page size to 1048576
Parquet dictionary page size to 1048576
Dictionary is true
Writer version is: PARQUET_1_0
Page size checking is: estimated
Min row count for page size check is: 100
Max row count for page size check is: 10000
Truncate length for column indexes is: 64
Truncate length for statistics min/max  is: 2147483647
Bloom filter enabled: false
Max Bloom filter size for a column is 1048576
Bloom filter expected number of distinct values are: null
Page row count limit to 20000
Writing page checksums is: on
22/02/19 10:51:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "properties",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_agent",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary event_name (STRING);
  optional binary event_time (STRING);
  optional binary properties (STRING);
  optional binary session_id (STRING);
  optional binary user_agent (STRING);
  optional binary uuid (STRING);
}


22/02/19 10:51:03 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/src/main/resources/seed, range: 0-1918, partition values: [empty row]
22/02/19 10:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_202202191051031419065000342749908_0006_m_000000_5' to file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/event_name_with_spaces/_temporary/0/task_202202191051031419065000342749908_0006_m_000000
22/02/19 10:51:03 INFO SparkHadoopMapRedUtil: attempt_202202191051031419065000342749908_0006_m_000000_5: Committed
22/02/19 10:51:03 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 2829 bytes result sent to driver
22/02/19 10:51:03 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 58 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:03 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
22/02/19 10:51:03 INFO DAGScheduler: ResultStage 6 (saveAsTable at TableSplitter.scala:35) finished in 0.087 s
22/02/19 10:51:03 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
22/02/19 10:51:03 INFO DAGScheduler: Job 5 finished: saveAsTable at TableSplitter.scala:35, took 0.089546 s
22/02/19 10:51:03 INFO FileFormatWriter: Start to commit write Job 6171715d-312e-45d1-9a6d-f6b2d7251b48.
22/02/19 10:51:03 INFO FileFormatWriter: Write Job 6171715d-312e-45d1-9a6d-f6b2d7251b48 committed. Elapsed time: 14 ms.
22/02/19 10:51:03 INFO FileFormatWriter: Finished processing stats for write job 6171715d-312e-45d1-9a6d-f6b2d7251b48.
22/02/19 10:51:03 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:03 INFO HiveExternalCatalog: Persisting file based data source table `mixedtiles`.`event_name_with_spaces` into Hive metastore in Hive compatible format.
22/02/19 10:51:03 INFO HiveMetaStore: 0: create_table: Table(tableName:event_name_with_spaces, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260663, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/event_name_with_spaces, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=create_table: Table(tableName:event_name_with_spaces, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260663, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/event_name_with_spaces, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:03 INFO log: Updating table stats fast for event_name_with_spaces
22/02/19 10:51:03 INFO log: Updated size of table event_name_with_spaces to 3136
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_name),EqualTo(event_name,order_completed)
22/02/19 10:51:03 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_name#7),(event_name#7 = order_completed)
22/02/19 10:51:03 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 189.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.1.101:52505 (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:03 INFO SparkContext: Created broadcast 11 from saveAsTable at TableSplitter.scala:35
22/02/19 10:51:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:03 INFO SparkContext: Starting job: saveAsTable at TableSplitter.scala:35
22/02/19 10:51:03 INFO DAGScheduler: Got job 6 (saveAsTable at TableSplitter.scala:35) with 1 output partitions
22/02/19 10:51:03 INFO DAGScheduler: Final stage: ResultStage 7 (saveAsTable at TableSplitter.scala:35)
22/02/19 10:51:03 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:03 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:03 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at saveAsTable at TableSplitter.scala:35), which has no missing parents
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 209.6 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 75.6 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.1.101:52505 (size: 75.6 KiB, free: 4.6 GiB)
22/02/19 10:51:03 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at saveAsTable at TableSplitter.scala:35) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:03 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
22/02/19 10:51:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes) taskResourceAssignments Map()
22/02/19 10:51:03 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:03 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
22/02/19 10:51:03 INFO ParquetOutputFormat: Validation is off
22/02/19 10:51:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/02/19 10:51:03 INFO ParquetOutputFormat: Parquet properties are:
Parquet page size to 1048576
Parquet dictionary page size to 1048576
Dictionary is true
Writer version is: PARQUET_1_0
Page size checking is: estimated
Min row count for page size check is: 100
Max row count for page size check is: 10000
Truncate length for column indexes is: 64
Truncate length for statistics min/max  is: 2147483647
Bloom filter enabled: false
Max Bloom filter size for a column is 1048576
Bloom filter expected number of distinct values are: null
Page row count limit to 20000
Writing page checksums is: on
22/02/19 10:51:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "properties",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_agent",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary event_name (STRING);
  optional binary event_time (STRING);
  optional binary properties (STRING);
  optional binary session_id (STRING);
  optional binary user_agent (STRING);
  optional binary uuid (STRING);
}


22/02/19 10:51:03 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/src/main/resources/seed, range: 0-1918, partition values: [empty row]
22/02/19 10:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_202202191051033834370570997475760_0007_m_000000_6' to file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/order_completed/_temporary/0/task_202202191051033834370570997475760_0007_m_000000
22/02/19 10:51:03 INFO SparkHadoopMapRedUtil: attempt_202202191051033834370570997475760_0007_m_000000_6: Committed
22/02/19 10:51:03 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 2829 bytes result sent to driver
22/02/19 10:51:03 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 58 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:03 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
22/02/19 10:51:03 INFO DAGScheduler: ResultStage 7 (saveAsTable at TableSplitter.scala:35) finished in 0.087 s
22/02/19 10:51:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
22/02/19 10:51:03 INFO DAGScheduler: Job 6 finished: saveAsTable at TableSplitter.scala:35, took 0.088597 s
22/02/19 10:51:03 INFO FileFormatWriter: Start to commit write Job 7c0e5c50-e7c1-4c3e-b840-f78a4f836bff.
22/02/19 10:51:03 INFO FileFormatWriter: Write Job 7c0e5c50-e7c1-4c3e-b840-f78a4f836bff committed. Elapsed time: 15 ms.
22/02/19 10:51:03 INFO FileFormatWriter: Finished processing stats for write job 7c0e5c50-e7c1-4c3e-b840-f78a4f836bff.
22/02/19 10:51:03 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:03 INFO HiveExternalCatalog: Persisting file based data source table `mixedtiles`.`order_completed` into Hive metastore in Hive compatible format.
22/02/19 10:51:03 INFO HiveMetaStore: 0: create_table: Table(tableName:order_completed, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260663, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/order_completed, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=create_table: Table(tableName:order_completed, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260663, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/order_completed, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:03 INFO log: Updating table stats fast for order_completed
22/02/19 10:51:03 INFO log: Updated size of table order_completed to 2839
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:03 INFO FileSourceStrategy: Pushed Filters: IsNotNull(event_name),EqualTo(event_name,event-name-with-dashes)
22/02/19 10:51:03 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(event_name#7),(event_name#7 = event-name-with-dashes)
22/02/19 10:51:03 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:03 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 189.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 4.6 GiB)
22/02/19 10:51:03 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.1.101:52505 (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:03 INFO SparkContext: Created broadcast 13 from saveAsTable at TableSplitter.scala:35
22/02/19 10:51:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:03 INFO SparkContext: Starting job: saveAsTable at TableSplitter.scala:35
22/02/19 10:51:03 INFO DAGScheduler: Got job 7 (saveAsTable at TableSplitter.scala:35) with 1 output partitions
22/02/19 10:51:03 INFO DAGScheduler: Final stage: ResultStage 8 (saveAsTable at TableSplitter.scala:35)
22/02/19 10:51:03 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:03 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:03 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at saveAsTable at TableSplitter.scala:35), which has no missing parents
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 209.6 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 75.7 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.1.101:52505 (size: 75.7 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at saveAsTable at TableSplitter.scala:35) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
22/02/19 10:51:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7847 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
22/02/19 10:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/02/19 10:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/02/19 10:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/02/19 10:51:04 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:04 INFO CodecConfig: Compression: SNAPPY
22/02/19 10:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
22/02/19 10:51:04 INFO ParquetOutputFormat: Validation is off
22/02/19 10:51:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/02/19 10:51:04 INFO ParquetOutputFormat: Parquet properties are:
Parquet page size to 1048576
Parquet dictionary page size to 1048576
Dictionary is true
Writer version is: PARQUET_1_0
Page size checking is: estimated
Min row count for page size check is: 100
Max row count for page size check is: 10000
Truncate length for column indexes is: 64
Truncate length for statistics min/max  is: 2147483647
Bloom filter enabled: false
Max Bloom filter size for a column is 1048576
Bloom filter expected number of distinct values are: null
Page row count limit to 20000
Writing page checksums is: on
22/02/19 10:51:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "properties",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "session_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_agent",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uuid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary event_name (STRING);
  optional binary event_time (STRING);
  optional binary properties (STRING);
  optional binary session_id (STRING);
  optional binary user_agent (STRING);
  optional binary uuid (STRING);
}


22/02/19 10:51:04 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/src/main/resources/seed, range: 0-1918, partition values: [empty row]
22/02/19 10:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_202202191051035457820246699388341_0008_m_000000_7' to file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/event_name_with_dashes/_temporary/0/task_202202191051035457820246699388341_0008_m_000000
22/02/19 10:51:04 INFO SparkHadoopMapRedUtil: attempt_202202191051035457820246699388341_0008_m_000000_7: Committed
22/02/19 10:51:04 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 2829 bytes result sent to driver
22/02/19 10:51:04 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 49 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:04 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
22/02/19 10:51:04 INFO DAGScheduler: ResultStage 8 (saveAsTable at TableSplitter.scala:35) finished in 0.079 s
22/02/19 10:51:04 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
22/02/19 10:51:04 INFO DAGScheduler: Job 7 finished: saveAsTable at TableSplitter.scala:35, took 0.081347 s
22/02/19 10:51:04 INFO FileFormatWriter: Start to commit write Job 2d4af736-310b-4b4d-93dc-dea1bb090e66.
22/02/19 10:51:04 INFO FileFormatWriter: Write Job 2d4af736-310b-4b4d-93dc-dea1bb090e66 committed. Elapsed time: 14 ms.
22/02/19 10:51:04 INFO FileFormatWriter: Finished processing stats for write job 2d4af736-310b-4b4d-93dc-dea1bb090e66.
22/02/19 10:51:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:04 INFO HiveExternalCatalog: Persisting file based data source table `mixedtiles`.`event_name_with_dashes` into Hive metastore in Hive compatible format.
22/02/19 10:51:04 INFO HiveMetaStore: 0: create_table: Table(tableName:event_name_with_dashes, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260663, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/event_name_with_dashes, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=create_table: Table(tableName:event_name_with_dashes, dbName:mixedtiles, owner:Elad.Gazit, createTime:1645260663, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:event_name, type:string, comment:null), FieldSchema(name:event_time, type:string, comment:null), FieldSchema(name:properties, type:string, comment:null), FieldSchema(name:session_id, type:string, comment:null), FieldSchema(name:user_agent, type:string, comment:null), FieldSchema(name:uuid, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/event_name_with_dashes, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={"type":"struct","fields":[{"name":"event_name","type":"string","nullable":true,"metadata":{}},{"name":"event_time","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":"string","nullable":true,"metadata":{}},{"name":"session_id","type":"string","nullable":true,"metadata":{}},{"name":"user_agent","type":"string","nullable":true,"metadata":{}},{"name":"uuid","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.2.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{Elad.Gazit=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:Elad.Gazit, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
22/02/19 10:51:04 INFO log: Updating table stats fast for event_name_with_dashes
22/02/19 10:51:04 INFO log: Updated size of table event_name_with_dashes to 3137
22/02/19 10:51:04 INFO TableSplitter: show tables in mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_tables: db=mixedtiles pat=*
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_tables: db=mixedtiles pat=*
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.1.101:52505 in memory (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO CodeGenerator: Code generated in 10.12817 ms
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.101:52505 in memory (size: 12.3 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.1.101:52505 in memory (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.1.101:52505 in memory (size: 75.7 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.1.101:52505 in memory (size: 75.6 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.1.101:52505 in memory (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.101:52505 in memory (size: 10.5 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.1.101:52505 in memory (size: 75.6 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.1.101:52505 in memory (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.1.101:52505 in memory (size: 75.7 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.101:52505 in memory (size: 32.8 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO CodeGenerator: Code generated in 10.060377 ms
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.101:52505 in memory (size: 6.3 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.101:52505 in memory (size: 75.6 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.101:52505 in memory (size: 32.7 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Starting job: show at TableSplitter.scala:56
22/02/19 10:51:04 INFO DAGScheduler: Got job 8 (show at TableSplitter.scala:56) with 1 output partitions
22/02/19 10:51:04 INFO DAGScheduler: Final stage: ResultStage 9 (show at TableSplitter.scala:56)
22/02/19 10:51:04 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:04 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:04 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[28] at show at TableSplitter.scala:56), which has no missing parents
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 7.7 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.1.101:52505 (size: 3.9 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[28] at show at TableSplitter.scala:56) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:04 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
22/02/19 10:51:04 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7683 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)
22/02/19 10:51:04 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1511 bytes result sent to driver
22/02/19 10:51:04 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 10 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:04 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
22/02/19 10:51:04 INFO DAGScheduler: ResultStage 9 (show at TableSplitter.scala:56) finished in 0.015 s
22/02/19 10:51:04 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
22/02/19 10:51:04 INFO DAGScheduler: Job 8 finished: show at TableSplitter.scala:56, took 0.017653 s
22/02/19 10:51:04 INFO SparkContext: Starting job: show at TableSplitter.scala:56
22/02/19 10:51:04 INFO DAGScheduler: Got job 9 (show at TableSplitter.scala:56) with 4 output partitions
22/02/19 10:51:04 INFO DAGScheduler: Final stage: ResultStage 10 (show at TableSplitter.scala:56)
22/02/19 10:51:04 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:04 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:04 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[28] at show at TableSplitter.scala:56), which has no missing parents
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.7 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.1.101:52505 (size: 3.9 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:04 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (MapPartitionsRDD[28] at show at TableSplitter.scala:56) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
22/02/19 10:51:04 INFO TaskSchedulerImpl: Adding task set 10.0 with 4 tasks resource profile 0
22/02/19 10:51:04 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9) (192.168.1.101, executor driver, partition 1, PROCESS_LOCAL, 7691 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10) (192.168.1.101, executor driver, partition 2, PROCESS_LOCAL, 7691 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 11) (192.168.1.101, executor driver, partition 3, PROCESS_LOCAL, 7683 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 12) (192.168.1.101, executor driver, partition 4, PROCESS_LOCAL, 7675 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
22/02/19 10:51:04 INFO Executor: Running task 1.0 in stage 10.0 (TID 10)
22/02/19 10:51:04 INFO Executor: Running task 2.0 in stage 10.0 (TID 11)
22/02/19 10:51:04 INFO Executor: Running task 3.0 in stage 10.0 (TID 12)
22/02/19 10:51:04 INFO Executor: Finished task 1.0 in stage 10.0 (TID 10). 1518 bytes result sent to driver
22/02/19 10:51:04 INFO Executor: Finished task 3.0 in stage 10.0 (TID 12). 1503 bytes result sent to driver
22/02/19 10:51:04 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 1518 bytes result sent to driver
22/02/19 10:51:04 INFO Executor: Finished task 2.0 in stage 10.0 (TID 11). 1511 bytes result sent to driver
22/02/19 10:51:04 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 10 ms on 192.168.1.101 (executor driver) (1/4)
22/02/19 10:51:04 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 12) in 10 ms on 192.168.1.101 (executor driver) (2/4)
22/02/19 10:51:04 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 11 ms on 192.168.1.101 (executor driver) (3/4)
22/02/19 10:51:04 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 11) in 11 ms on 192.168.1.101 (executor driver) (4/4)
22/02/19 10:51:04 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
22/02/19 10:51:04 INFO DAGScheduler: ResultStage 10 (show at TableSplitter.scala:56) finished in 0.016 s
22/02/19 10:51:04 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
22/02/19 10:51:04 INFO DAGScheduler: Job 9 finished: show at TableSplitter.scala:56, took 0.017339 s
22/02/19 10:51:04 INFO CodeGenerator: Code generated in 8.466002 ms
22/02/19 10:51:04 INFO TableSplitter: select * from mixedtiles.contact_support
+----------+----------------------+-----------+
|namespace |tableName             |isTemporary|
+----------+----------------------+-----------+
|mixedtiles|contact_support       |false      |
|mixedtiles|event_name_with_dashes|false      |
|mixedtiles|event_name_with_spaces|false      |
|mixedtiles|order_completed       |false      |
|mixedtiles|signup                |false      |
+----------+----------------------+-----------+

22/02/19 10:51:04 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=contact_support
22/02/19 10:51:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:04 INFO FileSourceStrategy: Pushed Filters:
22/02/19 10:51:04 INFO FileSourceStrategy: Post-Scan Filters:
22/02/19 10:51:04 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:04 INFO CodeGenerator: Code generated in 17.890354 ms
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 191.9 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.1.101:52505 (size: 33.6 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 17 from show at TableSplitter.scala:63
22/02/19 10:51:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:04 INFO SparkContext: Starting job: show at TableSplitter.scala:63
22/02/19 10:51:04 INFO DAGScheduler: Got job 10 (show at TableSplitter.scala:63) with 1 output partitions
22/02/19 10:51:04 INFO DAGScheduler: Final stage: ResultStage 11 (show at TableSplitter.scala:63)
22/02/19 10:51:04 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:04 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:04 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[32] at show at TableSplitter.scala:63), which has no missing parents
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 15.6 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.1.101:52505 (size: 6.2 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[32] at show at TableSplitter.scala:63) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:04 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
22/02/19 10:51:04 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 13) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7937 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO Executor: Running task 0.0 in stage 11.0 (TID 13)
22/02/19 10:51:04 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/contact_support/part-00000-f0400c16-3c51-400f-a2d7-6df9c824638c-c000.snappy.parquet, range: 0-2854, partition values: [empty row]
22/02/19 10:51:04 INFO CodecPool: Got brand-new decompressor [.snappy]
22/02/19 10:51:04 INFO Executor: Finished task 0.0 in stage 11.0 (TID 13). 1982 bytes result sent to driver
22/02/19 10:51:04 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 13) in 98 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:04 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
22/02/19 10:51:04 INFO DAGScheduler: ResultStage 11 (show at TableSplitter.scala:63) finished in 0.110 s
22/02/19 10:51:04 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
22/02/19 10:51:04 INFO DAGScheduler: Job 10 finished: show at TableSplitter.scala:63, took 0.111448 s
22/02/19 10:51:04 INFO CodeGenerator: Code generated in 11.737365 ms
22/02/19 10:51:04 INFO TableSplitter: select * from mixedtiles.signup
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=signup
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=signup
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=signup
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=signup
+---------------+-----------------------+----------+------------------------------------+------------------------------------------------------------------------+------------------------------------+
|event_name     |event_time             |properties|session_id                          |user_agent                                                              |uuid                                |
+---------------+-----------------------+----------+------------------------------------+------------------------------------------------------------------------+------------------------------------+
|contact_support|2021-02-01 03:00:00.000|json      |78b39b12-2496-4874-bbdd-d0092d52c010|Mozilla/5.0 (compatible; MSIE 5.0; Windows 98; Win 9x 4.90; Trident/3.0)|b4406077-e781-436e-94a2-f426742e0877|
+---------------+-----------------------+----------+------------------------------------+------------------------------------------------------------------------+------------------------------------+

22/02/19 10:51:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:04 INFO FileSourceStrategy: Pushed Filters:
22/02/19 10:51:04 INFO FileSourceStrategy: Post-Scan Filters:
22/02/19 10:51:04 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 191.9 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.1.101:52505 (size: 33.6 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 19 from show at TableSplitter.scala:63
22/02/19 10:51:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:04 INFO SparkContext: Starting job: show at TableSplitter.scala:63
22/02/19 10:51:04 INFO DAGScheduler: Got job 11 (show at TableSplitter.scala:63) with 1 output partitions
22/02/19 10:51:04 INFO DAGScheduler: Final stage: ResultStage 12 (show at TableSplitter.scala:63)
22/02/19 10:51:04 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:04 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:04 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[36] at show at TableSplitter.scala:63), which has no missing parents
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 15.6 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.1.101:52505 (size: 6.2 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[36] at show at TableSplitter.scala:63) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:04 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
22/02/19 10:51:04 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 14) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7928 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO Executor: Running task 0.0 in stage 12.0 (TID 14)
22/02/19 10:51:04 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/signup/part-00000-a1353902-17eb-4a8e-840a-0fd93a8a7625-c000.snappy.parquet, range: 0-2813, partition values: [empty row]
22/02/19 10:51:04 INFO Executor: Finished task 0.0 in stage 12.0 (TID 14). 1987 bytes result sent to driver
22/02/19 10:51:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 14) in 13 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:04 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
22/02/19 10:51:04 INFO DAGScheduler: ResultStage 12 (show at TableSplitter.scala:63) finished in 0.022 s
22/02/19 10:51:04 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
22/02/19 10:51:04 INFO DAGScheduler: Job 11 finished: show at TableSplitter.scala:63, took 0.023828 s
22/02/19 10:51:04 INFO TableSplitter: select * from mixedtiles.event_name_with_spaces
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_spaces
+----------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------+------------------------------------+
|event_name|event_time             |properties|session_id                          |user_agent                                                                  |uuid                                |
+----------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------+------------------------------------+
|signup    |2021-01-01 02:00:00.000|json      |8d2f6b98-6308-40a8-9fa5-96870288102f|Mozilla/5.0 (Windows NT 6.2; en-US; rv:1.9.0.20) Gecko/20200217 Firefox/37.0|83f69e5a-bd88-4dd1-bb0a-031d2689fda2|
+----------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------+------------------------------------+

22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_spaces
22/02/19 10:51:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
22/02/19 10:51:04 INFO FileSourceStrategy: Pushed Filters:
22/02/19 10:51:04 INFO FileSourceStrategy: Post-Scan Filters:
22/02/19 10:51:04 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 191.9 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.1.101:52505 (size: 33.6 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 21 from show at TableSplitter.scala:63
22/02/19 10:51:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:04 INFO SparkContext: Starting job: show at TableSplitter.scala:63
22/02/19 10:51:04 INFO DAGScheduler: Got job 12 (show at TableSplitter.scala:63) with 1 output partitions
22/02/19 10:51:04 INFO DAGScheduler: Final stage: ResultStage 13 (show at TableSplitter.scala:63)
22/02/19 10:51:04 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:04 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:04 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[40] at show at TableSplitter.scala:63), which has no missing parents
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.6 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 4.6 GiB)
22/02/19 10:51:04 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.1.101:52505 (size: 6.2 KiB, free: 4.6 GiB)
22/02/19 10:51:04 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[40] at show at TableSplitter.scala:63) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:04 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
22/02/19 10:51:04 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 15) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7944 bytes) taskResourceAssignments Map()
22/02/19 10:51:04 INFO Executor: Running task 0.0 in stage 13.0 (TID 15)
22/02/19 10:51:04 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/event_name_with_spaces/part-00000-78eb02b4-7c44-42ab-9563-161571c385b8-c000.snappy.parquet, range: 0-3136, partition values: [empty row]
22/02/19 10:51:04 INFO Executor: Finished task 0.0 in stage 13.0 (TID 15). 2042 bytes result sent to driver
22/02/19 10:51:04 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 15) in 17 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:04 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
22/02/19 10:51:04 INFO DAGScheduler: ResultStage 13 (show at TableSplitter.scala:63) finished in 0.026 s
22/02/19 10:51:04 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
22/02/19 10:51:04 INFO DAGScheduler: Job 12 finished: show at TableSplitter.scala:63, took 0.028815 s
22/02/19 10:51:04 INFO TableSplitter: select * from mixedtiles.order_completed
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=order_completed
+----------------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+
|event_name            |event_time             |properties|session_id                          |user_agent                                                                                                            |uuid                                |
+----------------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+
|event name with spaces|2022-01-01 04:00:00.000|json      |b82frta7-3a69-4106-b506-827c217ff1a1|Mozilla/5.0 (Macintosh; PPC Mac OS X 10_7_6) AppleWebKit/5311 (KHTML, like Gecko) Chrome/39.0.816.0 Mobile Safari/5311|eeeb1dcb-0844-4920-bbd4-3a67j4863f9d|
+----------------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+

22/02/19 10:51:04 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:04 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=order_completed
22/02/19 10:51:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:04 INFO FileSourceStrategy: Pushed Filters:
22/02/19 10:51:04 INFO FileSourceStrategy: Post-Scan Filters:
22/02/19 10:51:04 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:05 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 191.9 KiB, free 4.6 GiB)
22/02/19 10:51:05 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 4.6 GiB)
22/02/19 10:51:05 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.1.101:52505 (size: 33.6 KiB, free: 4.6 GiB)
22/02/19 10:51:05 INFO SparkContext: Created broadcast 23 from show at TableSplitter.scala:63
22/02/19 10:51:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:05 INFO SparkContext: Starting job: show at TableSplitter.scala:63
22/02/19 10:51:05 INFO DAGScheduler: Got job 13 (show at TableSplitter.scala:63) with 1 output partitions
22/02/19 10:51:05 INFO DAGScheduler: Final stage: ResultStage 14 (show at TableSplitter.scala:63)
22/02/19 10:51:05 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:05 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:05 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[44] at show at TableSplitter.scala:63), which has no missing parents
22/02/19 10:51:05 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 15.6 KiB, free 4.6 GiB)
22/02/19 10:51:05 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 4.6 GiB)
22/02/19 10:51:05 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 192.168.1.101:52505 (size: 6.2 KiB, free: 4.6 GiB)
22/02/19 10:51:05 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[44] at show at TableSplitter.scala:63) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:05 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
22/02/19 10:51:05 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7937 bytes) taskResourceAssignments Map()
22/02/19 10:51:05 INFO Executor: Running task 0.0 in stage 14.0 (TID 16)
22/02/19 10:51:05 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/order_completed/part-00000-abc42f9d-a954-41fa-84d3-1a234ee2cd22-c000.snappy.parquet, range: 0-2839, partition values: [empty row]
22/02/19 10:51:05 INFO Executor: Finished task 0.0 in stage 14.0 (TID 16). 2185 bytes result sent to driver
22/02/19 10:51:05 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 16 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:05 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
22/02/19 10:51:05 INFO DAGScheduler: ResultStage 14 (show at TableSplitter.scala:63) finished in 0.025 s
22/02/19 10:51:05 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
22/02/19 10:51:05 INFO DAGScheduler: Job 13 finished: show at TableSplitter.scala:63, took 0.028436 s
22/02/19 10:51:05 INFO TableSplitter: select * from mixedtiles.event_name_with_dashes
22/02/19 10:51:05 INFO HiveMetaStore: 0: get_database: mixedtiles
22/02/19 10:51:05 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_database: mixedtiles
22/02/19 10:51:05 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:05 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_dashes
+---------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+
|event_name     |event_time             |properties|session_id                          |user_agent                                                                                                            |uuid                                |
+---------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+
|order_completed|2021-01-01 01:00:00.000|json      |ea56ff1d-1cbd-4678-a06b-4ee7fff2d171|Mozilla/5.0 (Windows 95) AppleWebKit/5360 (KHTML, like Gecko) Chrome/36.0.828.0 Mobile Safari/5360                    |898b2e7e-9059-457e-9264-fe7a8fccaafc|
|order_completed|2022-01-01 04:00:00.000|json      |b826d4a7-3a69-4106-b506-827c217ff1a1|Mozilla/5.0 (Macintosh; PPC Mac OS X 10_7_6) AppleWebKit/5311 (KHTML, like Gecko) Chrome/39.0.816.0 Mobile Safari/5311|eeeb1dcb-0844-4920-bbd4-3acee4863f9d|
+---------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+

22/02/19 10:51:05 INFO HiveMetaStore: 0: get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:05 INFO audit: ugi=Elad.Gazit	ip=unknown-ip-addr	cmd=get_table : db=mixedtiles tbl=event_name_with_dashes
22/02/19 10:51:05 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
22/02/19 10:51:05 INFO FileSourceStrategy: Pushed Filters:
22/02/19 10:51:05 INFO FileSourceStrategy: Post-Scan Filters:
22/02/19 10:51:05 INFO FileSourceStrategy: Output Data Schema: struct<event_name: string, event_time: string, properties: string, session_id: string, user_agent: string ... 1 more field>
22/02/19 10:51:05 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 191.9 KiB, free 4.6 GiB)
22/02/19 10:51:05 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 4.6 GiB)
22/02/19 10:51:05 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 192.168.1.101:52505 (size: 33.6 KiB, free: 4.6 GiB)
22/02/19 10:51:05 INFO SparkContext: Created broadcast 25 from show at TableSplitter.scala:63
22/02/19 10:51:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/19 10:51:05 INFO SparkContext: Starting job: show at TableSplitter.scala:63
22/02/19 10:51:05 INFO DAGScheduler: Got job 14 (show at TableSplitter.scala:63) with 1 output partitions
22/02/19 10:51:05 INFO DAGScheduler: Final stage: ResultStage 15 (show at TableSplitter.scala:63)
22/02/19 10:51:05 INFO DAGScheduler: Parents of final stage: List()
22/02/19 10:51:05 INFO DAGScheduler: Missing parents: List()
22/02/19 10:51:05 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[48] at show at TableSplitter.scala:63), which has no missing parents
22/02/19 10:51:05 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.6 KiB, free 4.6 GiB)
22/02/19 10:51:05 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 4.6 GiB)
22/02/19 10:51:05 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.1.101:52505 (size: 6.2 KiB, free: 4.6 GiB)
22/02/19 10:51:05 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1478
22/02/19 10:51:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[48] at show at TableSplitter.scala:63) (first 15 tasks are for partitions Vector(0))
22/02/19 10:51:05 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
22/02/19 10:51:05 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 17) (192.168.1.101, executor driver, partition 0, PROCESS_LOCAL, 7944 bytes) taskResourceAssignments Map()
22/02/19 10:51:05 INFO Executor: Running task 0.0 in stage 15.0 (TID 17)
22/02/19 10:51:05 INFO FileScanRDD: Reading File path: file:///Users/Elad.Gazit/IdeaProjects/table_splitter/spark-warehouse/mixedtiles.db/event_name_with_dashes/part-00000-39665368-b630-4bfb-869f-a1a695a51350-c000.snappy.parquet, range: 0-3137, partition values: [empty row]
22/02/19 10:51:05 INFO Executor: Finished task 0.0 in stage 15.0 (TID 17). 2042 bytes result sent to driver
22/02/19 10:51:05 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 17) in 12 ms on 192.168.1.101 (executor driver) (1/1)
22/02/19 10:51:05 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
22/02/19 10:51:05 INFO DAGScheduler: ResultStage 15 (show at TableSplitter.scala:63) finished in 0.019 s
22/02/19 10:51:05 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
22/02/19 10:51:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
22/02/19 10:51:05 INFO DAGScheduler: Job 14 finished: show at TableSplitter.scala:63, took 0.021054 s
22/02/19 10:51:05 INFO SparkContext: Invoking stop() from shutdown hook
+----------------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+
|event_name            |event_time             |properties|session_id                          |user_agent                                                                                                            |uuid                                |
+----------------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+
|event-name-with-dashes|2022-01-01 04:00:00.000|json      |b82ftyt7-3a69-4106-b506-827c217ff1a1|Mozilla/5.0 (Macintosh; PPC Mac OS X 10_7_6) AppleWebKit/5311 (KHTML, like Gecko) Chrome/39.0.816.0 Mobile Safari/5311|eefgddcb-0844-4920-bbd4-3a67j4863f9d|
+----------------------+-----------------------+----------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------+

22/02/19 10:51:05 INFO SparkUI: Stopped Spark web UI at http://192.168.1.101:4040
22/02/19 10:51:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/02/19 10:51:05 INFO MemoryStore: MemoryStore cleared
22/02/19 10:51:05 INFO BlockManager: BlockManager stopped
22/02/19 10:51:05 INFO BlockManagerMaster: BlockManagerMaster stopped
22/02/19 10:51:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/02/19 10:51:05 INFO SparkContext: Successfully stopped SparkContext
22/02/19 10:51:05 INFO ShutdownHookManager: Shutdown hook called
22/02/19 10:51:05 INFO ShutdownHookManager: Deleting directory /private/var/folders/nw/3qtbss3x3nb2l_1wv2jltw1r0000gq/T/spark-3fa0f7b8-996f-4c7e-bb3b-689c2f83eca9

Process finished with exit code 0
